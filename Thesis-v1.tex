\documentclass{llncs}
\usepackage{llncsdoc}
\usepackage[utf8]{inputenc}
\usepackage[compatibility=false]{caption}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{dsfont}
\author{Amit Wolfenfeld\inst{1}}
\institute{Technion}
\title{Title}

\begin{document}
\maketitle

\begin{abstract}
In machine learning, the notion of multi-armed bandits refers
to a class of online learning problems, in which a learner (also called decision maker or agent)  is supposed to simultaneously explore and exploit a given set of choice alternatives in the course of a sequential decision process. 
In the standard setting, the agent learns from stochastic feedback in the form of real-valued rewards.

The Dueling Bandits setting is an online learning framework in which actions are restricted to stochastic comparisons between pairs of strategies.
It models settings were absolute rewards are difficult to estimate but pairwise preferences are readily available. 

In this paper we propose several new methods for the Dueling Bandit Problem. Our approach extends the Doubler and Sparring algorithm proposed on [???]. We show empirical results using real data from Microsoft Research's LETOR project.
\end{abstract}

\section{Introduction}
	Multi-armed bandit (MAB) algorithms have received considerable attention and have been studied quite intensely in machine learning in the recent past. 
	The great interest in this topic is hardly surprising, given that the MAB setting is not only theoretically challenging but also practically useful, as can be seen from their use in a wide range of applications. For example, MAB algorithms turned out to offer effective solutions for problems for example- in search engines [?], online advertisement [?], and recommendation systems [?].
	The multi-armed bandit problem, or bandit problem for short, is one of the simplest instances of the sequential decision making problem, in which a learner needs to select options from a given set of alternatives repeatedly in an online manner—referring to the metaphor of the eponymous gambling machine in casinos, these options are also associated with “arms” that can be “pulled”. 
	More specifically, the agent selects one option at a time and observes a numerical (an d typically noisy) reward signal providing information on the quality of that option. The goal of the learner is to optimize an evaluation criterion such as the error rate (the expected percentage of playing a suboptimal arm) or the cumulative regret (the expected difference between the sum of the rewards actually obtained and the sum of rewards that could have been obtained by playing the best arm in each round).
	To achieve the desired goal, the online learner has to cope with the famous exploration/exploitation dilemma [5, 14, 35]: It has to find a reasonable compromise between playing the arms that produced high rewards in the past (exploitation) and trying other, possibly even better arms the (expected) reward of which is not precisely known so far (exploration).
	The assumption of a numerical reward signal is a potential limitation of the MAB setting. In fact, there are many practical applications in which it is hard or even impossible to quantify the quality of an option on a numerical scale. 
	More generally, the lack of precise feedback or exact supervision has been observed in other branches of machine learning, too, and has led to the emergence of fields such as weakly supervised learning and preference learning [25]. 
	In the latter, feedback is typically represented in a purely qualitative way, namely in terms of pairwise comparisons or rankings. 
	Feedback of this kind can be useful in online learning, too, as has been shown in online information retrieval [28, 42]. 
	As another example, think of crowd-sourcing services like the Amazon Mechanical Turk, where simple questions such as pairwise comparisons between decision alternatives are asked to a group of annotators. 
	The task is to approximate an underlying target ranking on the basis of these pairwise comparisons, which are possibly noisy and partially inconsistent [17]. 
	Another application worth mentioning is the ranking of XBox gamers based on their pairwise online duels; the ranking system of XBox is called TrueSkill TM [26].
	Extending the multi-armed bandit setting to the case of preference-based feedback, i.e., the case in which the online learner is allowed to compare arms in a qualitative way, is therefore a promising idea. 
	And indeed, extensions of that kind have received increasing attention in the recent years. 
	The aim of this paper is to provide a survey of the state-of-the-art in the field of preference-based multi-armed bandits (PB-MAB). 
	After recalling the basic setting of the problem in Section 2, we provide an overview of methods that have been proposed to tackle PB-MAB problems in Sections 3 and 4. 
	Our main criterion for systematization is the assumptions made by these methods about the data-generating process or, more specifically, the properties of the pairwise comparisons between arms. 
	Our survey is focused on the stochastic MAB setup, in which feedback is generated according to an underlying (unknown but stationary) probabilistic process; we do not cover the case of an adversarial data-generating processes, although this setting has recently received a lot of attention, too [1, 15, 14].
\newpage

\section{Scientific Background}

	\subsection{Multi Armed Bandit}
	The multi armed bandit problem is a sequential decision problem, where a single agent faces a stochastic environment. 
	The name “bandit” stems from a well known type of gambling machines, where at each turn a player pulls a mechanic lever (an “arm”) and receives a reward with some (usually small) probability. 
	In this setting, the actions the agent performs are referred to as arm pulls. The arms belong to some finite set $X = {x_1,..,x_k}$, and we denote $|X| = K $. 
	Each arm $x_i \in {x_1,..,x_2}$ has some probability distribution over $[0, 1]$, with expectation $\mu_{x_i}$. 
	We assume, throughout the text, the existence of a unique best arm:
	$$ x^* = argmax_{x_i}(\mu_{x_i})$$
	At each time step $t \in 0,1,..,T$ the agent pulls some arm $x_i \in {x_1,..,x_2}$ and gets a random reward $u_t$, independently of all previous rewards. 
	For each arm $x_i$ and all time steps $t \geq 1$, we denote by $n_{x_i}(t)$ the number of times arm $x_i$ was pulled by time arm $x_i$ was pulled by time step $t$.
	\subsection{UBDB}
	To formalize the problem of learning from preferences, we consider the following interactive online learning model, which we call the Utility-Based Dueling Bandits Problem (UBDB) similar to (Yue et al., 2012, Yue and Joachims, 2011). 
		At each iteration $t$, the learning system presents two actions $x_t, y_t \in X$ to the user, where $X$ is the set (either finite or infinite) of possible actions. 
		Each of the two actions has an associated random reward (or utility) for the user, which we denote by $u_t$ and $v_t$, respectively.
		The quantity $u_t$ (resp. $v_t$) is drawn from a distribution that depends on $x_t$ (resp. $y_t$) only.  
		We assume these utilities are in $[0, 1]$. The learning system is rewarded the average utility$ U_av(t) = (u_t + v_t)/2$ of the two actions it presents, but it does not observe this reward. 
		Instead, it only observes the user’s binary choice among the two alternative actions $xt_, y_t$, which depends on the respective utilities $u_t$ and $v_t$. 
		In particular, we model the observed choice as a $\{0, 1\}$ valued random variable $b_t$ distributed as
		\begin{equation}
		\begin{cases}
		P(b_t = 1|u_t, v_t) = \phi(u_t, v_t)
		\\
		P(b_t = 0|u_t, v_t) = \phi(v_t, u_t)
		\end{cases}
		\end{equation}			
	where $\phi:[0, 1] \times [0, 1] \rightarrow [0, 1]$ is a link function. 
	Clearly, the link function has to satisfy $\phi(A, B) + \phi(B, A) = 1$.
	The binary choice is interpreted as a stochastic preference response between the left alternative $x_t$ (if $b_t = 0$) and the right alternative $y_t$ (if $b_t = 1$). 
	The utility $U_{av}$ captures the overall latent user experience from the pair of alternatives. 
	In the utility based dueling bandit game (UBDB), the algorithm chooses $(x_t, y_t) \in X \times X$ at each step, and a corresponding pair of random utilities $(u_t, v_t) \in [0, 1]$ are given rise to, but not observed by the algorithm.  
	We assume $u_t$ is drawn from a distribution of expectation $\mu(x_t)$ and $v_t$ independently from a distribution of expectation $\mu(y_t)$. 
	The algorithm observes a choice variable $b_t \in \{0, 1\}$ distributed according to the law (1.1). 
	This random variable should be thought of as the outcome of a duel, or match between $x_t$ and $y_t$. 
	The outcome $b_t = 1$ (resp. $b_t = 0$) should be interpreted as “$y_t$ is chosen’ (resp. “$x_t$ is chosen”).
	The link function $\phi$, which is assumed to be known, quantitatively determines how to translate the utilities $u_t, v_t$ to winning probabilities. 
	The linear link function $\phi_{lin}$ is defined by
	$$P(b_t = 1|u_t, v_t) = \phi_{lin}(u_t, v_t) = \frac{1+v_t-u_t}{2}\in [0,1]$$
The unobserved reward is $U_{av}(t) = (u_t + v_t)/2$, and the corresponding cumulative utility based regret after $T$ steps is $R_U(T) = \sum_{t=1}^T \mu(x^*)-U_{av}(t)$, where $x^* = argmax_{x \in X} \mu(x)$. 
This implies that expected zero regret is achievable by setting $(x_t, y_t) = (x^*,x^*)$.
In practice, these two identical alternatives would be displayed as one, as would naturally happen in interleaved retrieval evaluation (Chapelle et al., 2012). 
It should be also clear that playing $(x^*,x^*)$ is pure exploitation, because the feedback is then an unbiased coin with zero exploratory information.
	
	\subsection{Dueling Bandits}
		The stochastic MAB problem with pairwise comparisons as actions has been studied under the notion of “dueling bandits” in several papers [?].
		However, since this term is associated with specific modeling assumptions, we shall use the more general term “preference-based bandits” throughout this paper.
		Consider a fixed set of arms $X = {x_1,..,x_k}$. 
		As actions, the learning algorithm (or simply the learner or agent) can perform a comparison between any pair of arms $x_i$ and $x_j$ , i.e., the action space can be identified with the set of index pairs $(i, j)$ such that $1 \leq i \leq j \leq K$. 
		We assume the feedback observable by the learner to be generated by an underlying (unknown) probabilistic process characterized by a preference relation
		$$P = [p_{x_i,x_j}] \in [0,1]^{K \times K} $$
		More specifically, for each pair of actions $(x_i ,x_j)$, this relation specifies the probability
		\begin{equation}
		Pr(x_i \succeq x_j) = p_{x_i,x_j}
		\end{equation}
		of observing a preference for $x_i$ in a direct comparison with $x_j$. 
		Thus, each $p_{x_i,x_j}$ specifies a Bernoulli distribution.
		These distributions are assumed to be stationary and independent, both across actions and iterations. 
		Thus, whenever the learner takes action $(x_i ,x_j)$, the outcome is distributed according to (1), regardless
of the outcomes in previous iterations.
		The relation $P$ is reciprocal in the sense that $p_{i,j} = 1-p_{j,i}$ for all $i, j \in [K] = \{1, . . . , K\}$.  
		We note that, instead of only observing strict preferences, one may also allow a comparison to result in a tie or an indifference. 
		In that case, the outcome is a trinomial instead of a binomial event. 
		Since this generalization makes the problem technically more complicated, though without changing it conceptually, we shall not consider it further. 
		In [?], indifference was handled by giving “half a point” to both arms, which, in expectation, is equivalent to deciding the winner by flipping a coin. 
		Thus, the problem is essentially reduced to the case of binomial outcomes.
		We say arm $x_i$ beats arm $x_j$ if $p_{i,j} > 1/2$, i.e., if the probability of winning in a pairwise comparison is larger for $x_i$ than it is for $x_j$ . 
		Clearly, the closer $p_{i,j}$ is to $1/2$, the harder it becomes to distinguish the arms $x_i$ and $x_j$ based on a finite sample set from $Pr(x_i \succeq x_j)$.
		 In the worst case, when $p_{i,j} = 1/2$, one cannot decide which arm is better based on a finite number of pairwise comparisons. 
		 Therefore,
		 $$ \Delta_{i,j} = p_{i,j} - 1/2$$
		 appears to be a reasonable quantity to characterize the hardness of a Deuling Bandits task (whatever goal the learner wants to achieve).
		 Note that $\Delta_{i,j}$ can also be negative (unlike the value-based setting, in which the quantity used for characterizing the complexity of a multi-armed bandit task is always positive and depends on the gap between the means of the best arm and the suboptimal arms).
		 
	\subsection{Probability estimation}		 
		 The decision making process iterates in discrete steps, either through a finite time horizon or an infinite horizon. 
		 As mentioned above, the learner is allowed to compare two actions in each iteration $t \in T$. 
		 Thus, in each iteration $t$, it selects an index pair $1 \leq i(t) \leq j(t) \leq K$ and observes
	\begin{equation}
		\begin{cases}
    		x_{i(t)} \succeq x_{j(t)} & \text{with probablity } p_{i,j} 
       	\\
    		x_{j(t)} \succeq x_{i(t)} & \text{with probablity } p_{j,i}
	\end{cases}
	\end{equation}		
	The pairwise probabilities $q_{i,j}$ can be estimated on the basis of finite sample sets. 
	Consider the set of time steps among the first $t$ iterations, in which the learner decides to compare arms $x_i$ and $x_j$ , and denote the size of this set by $n_{i,j}$ . 
	Moreover, denoting by $w_{i,j}$ and $w_{j,i}$ the number of “wins” of $x_i$ and $x_j$, respectively, the proportion of wins of $x_i$ against $x_j$ up to iteration $t$ is then given by
	$$ 
		\hat{P}_{x_i, x_j} = \frac{w_{i,j}}{n_{i,j}} = 
		\frac{w_{i,j}}{w_{i,j}+w_{j,i}}
	$$
	Since our samples are assumed to be independent and identically distributed (i.i.d), $p_{i,j}$ is a plausible estimate of the pairwise probability (1). 
	Yet, this estimate might be biased, since $n_{i,j}$ depends on the choice of the learner, which in turn depends on the data; therefore, $n_{i,j}$ itself is a random quantity. 
	A high probability confidence interval for $p_{i,j}$ can be obtained based on the Hoeffding bound [27], which is commonly used in the bandit literature. 
	Although the specific computation of the confidence intervals may differ from case to case, they are generally of the form $[p_{i,j} \pm c_{i,j} ]$. 
	Accordingly, if $p_{i,j} + c_{i,j} > 1/2]$, arm $x_{i}$ beats arm $x_{j}$ with high probability; analogously, $x_{i}$ is beaten by arm $x_{j}$ with high probability, if $p_{i,j} + c_{i,j} < 1/2]$.
	
	In a preference-based setting, defining a reasonable regret is not as straightforward as in the utility-based setting, where the sub-optimality of an action can be expressed easily on a numerical scale. 
	In particular, since the learner selects two arms to be compared in an iteration, the sub-optimality of both of these arms should be taken into account.
	A commonly used definition of regret is the following []. Suppose the learner selects arms $x_{i(t)}$ and $x_{j(t)}$ at time $t$. The cumulative regret incurred by the learner up to time $T$ is:
	\begin{equation}
	R_P(T) = \sum_{t=1}^T \frac{\Delta_{i^*,i(t)}+\Delta_{i^*,j(t)}}{2} 
	\end{equation}	 
	
	We will show that using the definition of the linear link function we both utility based regret and preference based regret are the same (till a factor of 2): 
	\begin{equation}\label{eq:ref1}
		p_{i^*,k} = \phi_{lin}(\mu(x^*),\mu(x_k)) = \frac{1 +\mu(x_k)-\mu(x^*)}{2}
	\end{equation}
	
	Incorporating \eqref{eq:ref1} in the definition of $\Delta_{i^*,k}$ we get
	$$
	\Delta_{i^*,k} = p_{i^*,k} - \frac{1}{2} = \frac{\mu(x^*)-\mu(x_k)}{2}
	$$
	And so the total regret is defined:
	$$ R_P(T) = \sum_{t=1}^T \frac{\Delta_{i^*,i(t)}+\Delta_{i^*,j(t)}}{2} =  
\sum_{t=1}^T \frac{\frac{\mu(x^*)-\mu(x_{i(t)})}{2}+\frac{\mu(x^*)-\mu(x_{j(t)})}{2}}{2} =
$$

$$
\frac{1}{2} \sum_{t=1}^T \mu(x^*) -\frac{
	\mu(x_{i(t)})+\mu(x_{j(t)})}{2} =
\frac{1}{2} \sum_{t=1}^T \mu(x^*)- U_{av} = \frac{1}{2}R_U(T)$$

\newpage

For the convenience of the reader we have included a table of all the definitions. 
	\begin{table}[h]
		\begin{tabular}{ll}
 			$T$ & Horizon \\
 			$t$ &  Round \\
 			$X$ & Arms space \\
 			$K = |X|$ & Total Number of Arms\\
 			$x_t \in X$ & Left arm \\
 			$y_t \in X$ & Right arm \\
 			$u_t \in \mu_t$ & Left utility \\
 			$v_t \in \mu_t$ & Right utility \\
 			$b_t$ & Observed feedback \\
 			$R_U(T)$ & Total Utility Based Regret till T\\
			$R_P(T)$ & Total Preference Based Regret till T\\
 			$\mathcal{P}$ & Set of potential arms \\
 			$\hat{P}_{x, y}$ & Estimate of $P(x>y)$\\
 			$\hat{C}_{x, y}$ &   Confidence interval of - $(\hat{P}_{x, y} - \sqrt{log(1/\delta)/t},\hat{P}_{x, y} +\sqrt{log(1/\delta)/t})$\\
 			$n_x$ &   The number of times arm $x$ has been played\\
 			$w_x$ & The number of times arm $x$ has won\\
 			$\hat{P}_x  $ &  $ w_x / n_x $\\
 			$ W = [w_{x,y}]$ & Number of wins of arm $x$ over arm $y$\\
 			$U = [u_{x,y}]$ &  Utility function\\
 			$\Theta_{x,y}$ &   Random variable
		\end{tabular}
	\end{table}

\newpage

\section{Related Work}
	We start by surveying dueling bandit algorithms. In this section we have included Interleaved Filtering, Beat the Mean Bandit, RUCB, RCS, SAVAGE and went into greater detail for the Sparring and Doubler algorithms they served as a foundation for our new approach.
	
\subsubsection{Interleaved Filter}
	(Yue et al.,2012)Choose candidate bandit at random. Makes a noisy comparisons (Bernoulli trial) against all other bandits. 
	IF maintains a mean and confidence interval for each pair of bandits being compared. Any arm that is empirically inferior to the candidate arm with  enough confidence is removed.
	Once the current candidate is removed and the new arm becomes the new candidate. This process continues until one arm is left.
\\
Yue et al. [44] propose an explore-then-exploit algorithm. 
The exploration step consists of a simple sequential elimination strategy,
called Interleaved Filtering (IF), which identifies the best arm with probability at least $1-\delta $ . The IF algorithm successively selects an arm which is compared to other arms in a one-versus-all manner. More specifically, the currently selected arm a i is compared to the rest of the active (not yet eliminated) arms. 
If an arm $x_j$ beats $x_i$ , that is, $q_{i,j}+c_{i,j}<1/2$, then $x_i$ is eliminated, and $x_j$ is compared to the rest of the (active) arms, again in a one-versus-all manner.
In addition, a simple pruning technique can be applied: if $q_{i,j}-c_{i,j}<1/2$ for
an arm $x_j$ at any time, then $x_j$ can be eliminated, as it cannot be the best arm any-more (with high probability). After the exploration step, the exploitation step simply takes the best arm $x^*$ found by IF and repeatedly compares $x^*$ to itself.	
	
	\input{InterleavedFilter.tex}

\newpage

\subsubsection{Beat The Mean Bandit}	
	Yue and Joachims [?] propose a preference-based online learning algorithm called Beat-The-Mean (BTM), which is an elimination strategy resembling IF. 
	However, while IF compares a single arm to the rest of the (active) arms in a one-versus-all manner, BTM selects an arm with the fewest comparisons so far and pairs it with a randomly chosen arm from the set of active arms (using the uniform distribution). 
	Based on the outcomes of the pairwise comparisons, a score $\hat{P}_{x_i}$ is assigned to each active arm $x_i$ , which is an empirical estimate of the probability that $x_i$ is winning in a pairwise comparison (not taking into account which arm it was compared to).
	The idea is that comparing an arm $x_i$ to the “mean” arm, which beats half of the arms, is equivalent to comparing $x_i$ to an arm randomly selected from the active set. One can deduce a confidence interval for the $\hat{P}_{x_i}$ scores, which allows for deciding whether the scores for two arms are significantly different. 
	An arm is then eliminated as soon as there is another arm with a significantly higher score.
	\input{BeatTheMean.tex}

\newpage

\subsubsection{RUCB} Relative Upper Confidence Bound
	Zoghi et al. [?], the well-known UCB [5] algorithm is adapted from the value-based to the preference- based MAB setup. 
	One of the main advantages of the proposed algorithm, is that only the existence of a Condorcet winner is required. 
	Consequently, it is more broadly applicable. The RUCB algorithm is based on the “optimism in the face of uncertainty” principle, which means that the arms to be compared next are selected based on the optimistic estimates of the pairwise probabilities, that is, based on the upper boundaries $u_{x_i,x_j}$ of the confidence intervals. 
	In an iteration step, RUCB selects the set of potential Condorcet winners for which all $u_{x_i,x_j}$ values are above $1/2$, and then selects an arm $x_i$ from this set uniformly at random. 
	Finally, $x_i$ is compared to the arm $x_j = argmax_{x} u_{x,x_j}$, , that may lead to the smallest regret, taking into account the optimistic estimates. 
	In the analysis of the RUCB algorithm, horizonless regret bounds are provided, both for the expected regret and high probability bound. Thus, unlike the bounds for IF and BTM, these bounds are valid for each time step. 
	Both the expected regret bound and high probability bound of RUCB are $O(K log T)$.
	However, while the regret bounds of IF and BTM only depend on $ min_{j \neq i^*}  \Delta_{i^*,j}$, the constants are now of different nature, despite being still calculated based on the $\Delta_{i,j}$  values.
	Therefore, the regret bounds for RUCB are not directly comparable with those given for IF and BTM.
	\input{RUCB.tex}
	\newpage
\subsubsection{RCS}
	(Munos et al., 2014) Relative Confidence Sampling aims to reduce cumulative regret by being less conservative than the existing methods about eliminating arms from contention. 
	Though designed for the ongoing regret minimization setting, RCS is also easily adapted to the explore-then-exploit setting. 
	While RCS is related to RUCB, which is also designed for the ongoing regret minimization setting, it differs in one crucial respect: the use of sampling when conducting a round-robin tournament to select a champion.
	The goal in doing so is to exploit one of the key lessons that has been learned in the study of regular K-armed bandits: that much better performance can be obtained by maintaining posterior distributions over the expected value of each arm and sampling from those posteriors to determine which ranker to select. 
	This is evidenced by the superior performance of Thompson Sampling [?], a K-armed bandit method that employs such sampling, over various UCB algorithms [?]. 
	Unlike UCB algorithms, which rely on estimates of the expected value that can be far off, sampling-based methods rely on posteriors that tend not to gravitate toward the extremes, leading to more appropriate choices more often.

	\input{RCS.tex}
	
\subsubsection{SAVAGE}
	(Uryoy et al., 2013) works by reducing a box-shaped confidence set until a single arm remains.
	Sensitivity analysis of variables for generic exploration (SAVAGE) is a more recent algorithm that performs better than both IF and BTM by a wide margin when the number of arms is small. 
	SAVAGE compares pairs of arms uniformly randomly until
there exists a pair for which one of the arms beats the other by
a wide margin, in which case the loser is removed from the pool of arms under consideration.
\input{SAVAGE.tex}

\subsubsection{Sparring}
	(Ailon et al., 2014) works by drawing a pair of arms from two SBM's in each round and comparing them as all the previous algorithms. Once observing the preference the algorithm updates the preferred (according to the observation) SBM.  
	
		\input{Sparring.tex}
	\begin{conjecture}
 		In the paper Ailon et al., 2014 conjectured that the utility based regret of the Sparring algorithm is bounded by the combined regret of the SBM's, with a possibility of a small overhead [?].
 	\end{conjecture}		
		
	\subsubsection{Sparring with Thompson Sampling}
		In our work  we have experimented with both SBMs black and white boxes. The configuration that showed the best result with minimum regret, is the following: 
		\input{SparringTS.tex}
		
	\subsubsection{Doubler}
(Ailon et al., 2014) handles a large or possibly infinite set of arms $X$. 
\\
This algorithm is best explained by thinking of a competition between
two players; the first controls the choice of the left arm and
the second player controls the right arm. The objective of each
player is to win as many rounds possible.
\\
This algorithm divides the time axis to exponentially growing epochs (First epoch is 2 rounds, second epoch is 4, third epoch is 8 and so forth).
In each epoch, the left player plays according to some fixed
(stochastic) strategy, while the right one plays adaptively according to a strategy provided by a SBM. At the beginning of a new epoch the distribution governing the left arm changes in a way that mimics the actions of the right arm in the previous epoch.\\
		\input{Doubler.tex}	
		\\
		In the finite case, one may set the SBM S to the standard
UCB, and obtain according to Corollary 3.3 the regret of Doubler is at most $O(H \log^2 (T))$ where $H=\sum_{i=1}^K \Delta^{-1}_i$.
\newpage

\section{Our Approach}	
		
	\subsubsection{Improved Doubler}
	The main drawback of the Doubler algorithm is that in each epoch the SBM is reset and as a result all the acquired data is lost. Therefore adding an extra $log(T)$ factor to the total regret.\\
	Instead of initializing the right arm’s SBM at each epoch and losing all the data, we will store this data.
Let $D_p$ denote the distribution of the left arm in the $p$’th epoch. 
By knowing $f_p := E_{x\in D_p} \mu(x)/2$, and by using $b_t + f_p$ as feedback on the right side we could separate $y_t$ from $x_t$ in the feedback.
\\
		\input{ImprovedDoubler.tex}
		
		$f_p$ is defined by $f_{p+1} = 
		\frac{1}{|T_p|}\sum\limits_{t\in T_p} \mu(y_t)/2$\\
		and so: $f_{p+1} = 
		\frac{1}{|T_p|}\sum\limits_{t\in T_p} E[b_t - \frac{1-\mu(x_t)}{2}] = \frac{1}{|T_p|}\sum\limits_{t\in T_p} E[b_t - f_{p-1}-\frac{1}{2}]$ \\
		We need to estimate $f_p$ with $\hat{f}_p$ and so we get

        $$\hat{f}_i=\hat{f}_{i-1} + \frac{\sum\limits_{t\in T_i} (2b_t -1)}{|T_{i-1}|}$$
		\\		
		Let's break it down a bit:\\
		$\hat{f}_{i+1}=\hat{f}_{i} + 
		\frac{\sum\limits_{t\in T_i} (2b_t -1)}{|T_{i}|} = 
		\hat{f}_{i} + 
		\frac{\sum\limits_{t\in T_i} (2b_t -1)}{2^{i}} = 
		\hat{f}_{i} + 
		\frac{\sum\limits_{t\in T_i} 2b_t}{2^{i}}-1 = 
		\hat{f}_{i} + B_i -1 =  
		\hat{f}_{i-1} +B_{i-1} -1+ B_i -1 = ... =  
		\hat{f}_{0} + \sum\limits_{j=1}^i B_{j}+i$
		\\
		From her we can bound $\hat{f}_{i}$.
		First lets look at $\sum\limits_{j=1}^i B_{j}$, this equals $E[b]$ which is bounded by $[0,1]$ and $i$ which is $log(T)$. And so we can bound $\hat{f}$ by $\log(T)$.

				
	\subsubsection{Unforgetful Thompson Sampling Doubler}
		Another improvement of the Doubler algorithm is the Unforgetful Thompson Sampling Doubler algorithm - here we do not reset the SBM in each epoch and use the Thompson Sampling method. This approach outperformed all the other algorithms apart from the Thompson Sampling Sparring approach mentioned in section 3.

		\input{ImprovedDoublerTS.tex}
		
		\newpage	
	\subsubsection{Balanced Doubler}
		Another improvement of the Doubler algorithm is the Balanced Doubler algorithm.In this algorithm we assume we know $\Delta$ ($\Delta = \mu(x_1) - \mu(x_2)$ were $x_1$ is the arm with the highest estimated reward and $x_2$ is the arm with the second highest estimated reward.
		\input{BalancedDoubler.tex}\\
		Proof of $log(T)$ regret:
		\\
		Lets define : $Y_i(t) = \frac{1}{p}\sum\limits_{j=1}^{p}\frac{1}{n_{i,j}}
						   \sum\limits_{t\in T_{i,j}} b_t$		
		\\
		Where $n_{i,j}$ is the number of time arm $i$ is played in epoch $j$.\\
		$X_i = Y_i (b_t = b_t- f_j) =   \frac{1}{p}\sum\limits_{j=1}^{p}\frac{1}{n_{i,j}}
						   \sum\limits_{t\in T_{i,j}} b_t-f_j$	
		\\
		Now let's look at $\mathbbm{E}(e^{\lambda X_i})$
		\\	
		$\mathbbm{E}(e^{\lambda X_i}) = \mathbbm{E}(e^{\lambda\cdot(
		\frac{1}{p}\sum\limits_{j=1}^{p}\frac{1}{n_{i,j}}
						   \sum\limits_{t\in T_{i,j}}( b_t-f_j
						   )}) =						      	   
						   \prod\limits_{j=1}^{p}
						   \left(
						   \mathbbm{E}\left[e^{\frac{\lambda}{p}\cdot\left(
						   	   \sum\limits_{t\in T_{i,j}}
						   	   		\frac{(b_t-f_j)}{n_{i,j}}
						\right)}\right]
						   \right)=
						   $		
						   \\
						   $ =						      	   
						   \prod\limits_{j=1}^{p}
						   \prod\limits_{t\in T_{i,j}}
						   \left(
						   \mathbbm{E}\left[e^{\frac{\lambda}
						   {p}\cdot\left(
						   	   		\frac{(b_t-f_j)}
						   	   		{n_{i,j}}\right)}
						   	   		\right]
						   \right)$
		Assuming that $\frac{1}{p}\cdot\left(\frac{b_t-f_j}
		{n_{i,j}}\right) $ is bounded by 1 and has an average of 0, we can say that
		\\
		$$ \prod\limits_{j=1}^{p}
						   \prod\limits_{t\in T_{i,j}}
						   \left(
						   \mathbbm{E}\left[e^{\frac{\lambda}
						   {p}\cdot\left(
						   	   		\frac{b_t-f_j}
						   	   		{n_{i,j}}\right)}
						   	   		\right]
						   \right) \leq
						   \prod\limits_{j=1}^{p}
						   \prod\limits_{t\in T_{i,j}}
						   \left(
						   e^{\frac{\lambda^2}{p^2\cdot n^2_{i,j}}}
						   \right)
						   $$
						   
						  There are $n_{i,j}$ plays in $T_{i,j}$, and so we get:
						  \\
						  $\prod\limits_{j=1}^{p}
						   \prod\limits_{t\in T_{i,j}}
						   \left(
						   e^{\frac{\lambda^2}{p^2\cdot n^2_{i,j}}}
						   \right) = 
						   \prod\limits_{j=1}^{p}						   
						   \left(
						   e^{\frac{\lambda^2}{p^2\cdot n_{i,j}}}
						   \right) = 
						   e^{
						   \frac{\lambda^2}{p^2}
						   \sum\limits_{j=1}^{p} \frac{1}{n_{i,j}}
						   }$
						
		And now for the main point, we wish to show that 
		$Pr[X_j > a] < \frac{1}{t}$\\
		$Pr[X_j > a] = Pr[e^{X_j} < e^a] \leq e^{
						   \frac{\lambda^2}{p^2}
						   \sum\limits_{j=1}^{p} \frac{1}{n_{i,j}}
						   - \lambda\cdot a}$
		\\
		\\
		$\lambda = \frac{a}{\frac{2}{p^2}\left( \sum\limits_{j=1}^{p} \frac{1}{n_{i,j}} \right)} $ 
		\\
		with some reverse engineering and in order to get $Pr[X_j > a] < \frac{1}{t}$ we get that $a = \sqrt{\frac{\log(t) \sum\limits_{j=1}^{p} \frac{1}{n_{i,j}}}{p^2}}$ 
 
 
	
		
\section{Experiments}

To evaluate our algorithms, we apply them to the problem of
ranker evaluation from the field of information retrieval
(IR) (Manning et al., 2008). A ranker is a
function that takes a user's search query and
ranks the documents in a collection according to their
relevance to that query. Ranker evaluation aims to determine
which set of rankers performs best.
\\
An effective way to achieve this is to use interleaved
comparisons (Radlinski et al., 2008), which interleave
the documents proposed by two different rankers and
presents the resulting list to the user, whose resulting
click feedback is used to infer a stochastic preference
for one of the rankers. Given a set of K rankers, the
problem of finding the best ranker can then be modelled 
as a K-armed Dueling Bandit problem, with each
arm relating to a ranker.
\\
Our setup is built on real IR data,
namely the LETOR MQ2007 dataset (Liu et al., 2007).
Using this data set, we create a set of 46 rankers, each
relating to a ranking feature provided in the
data set, e.g., PageRank. The ranker evaluation task
then corresponds to conclude which single feature
is the best ranker (Hofmann et al., 2013).
\\
We evaluated our algorithms and RCS
using randomly chosen subsets from the pool of 46
rankers, yielding K-armed dueling bandit problems
with $K\in \{7, 16, 46\}$. 
\\
We will present the utility based regret and the general regret (as define in section 2.1 and 2.2) on the MQ2007. 
\newpage

\subsection{Results}
We will start with displaying the results for 7 arms.
From Figure 1 we can clearly see that the SAVAGE and Doubler algorithms are outperformed by the other algorithms and so will be left out from the next figures.
\begin{figure}[h!]
\centering
  \includegraphics[scale=0.5]{figures/all_MQ2007_7arms.png}
  \caption{Utility Based Regret with 7 Arms}
\end{figure}

For a larger number of arms we can see that RCS and RUCB are outperformed by the other algorithms.

\begin{figure}[h!]
\centering
  \includegraphics[scale=0.5]{figures/all_MQ2007_24arms.png}
  \caption{Utility Based Regret with 24 Arms}
\end{figure}

And for 46 arms we can see that Sparring with Thompson Sampling methods performs better than all the other algorithms.


\subsection{Sparring}

We can see the clear advantage of using the Sparring algorithm over the RCS algorithm when dealing with a large number of arms.
\begin{figure}[h!]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.3]{figures/rcs_sparring_MQ2007_7arms.png}
  \caption{7 Arms}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.3]{figures/rcs_sparring_MQ2007_16arms.png}
  \caption{16 Arms}
  \label{fig:sub2}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.3]{figures/rcs_sparring_MQ2007_46arms.png}
  \caption{46 Arms}
  \label{fig:sub2}
\end{subfigure}
\caption{RCS Versus Sparring with Utility Based Regret}
\label{fig:test}
\end{figure}


With a small number of arms (Fig 1.) we can observe that the RCS algorithm quickly converges to the optimal arm.
\\
With a larger number of arms we can see that Sparring out-performs the RCS algorithm. 
\\
Now lets look at the general regret (as defined section 2.2). Here we used 46 arms:
\begin{figure}[h!]
  \centering
     \includegraphics[scale=0.4]{figures/rcs_sparring_MQ2007_general.png} 
  \caption{RCS Versus Sparring with Preference Based Regret with 46 Arms}
\end{figure}


\newpage
\subsection{Doubler}
Now lets have a look at the Doubler algorithm using the same data set.
\begin{figure}[h!]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.3]{figures/doubler_sparring_MQ2007_7arms.png}
  \caption{7 Arms}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.3]{figures/doubler_sparring_MQ2007_16arms.png}
  \caption{16 Arms}
  \label{fig:sub2}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.3]{figures/doubler_sparring_MQ2007_46arms.png}
  \caption{46 Arms}
  \label{fig:sub2}
\end{subfigure}
\caption{Doubler Versus Sparring with Utility Based Regret}
\label{fig:test}
\end{figure}

The general regret:
\begin{figure}[h!]
  \centering
     \includegraphics[scale=0.2]{figures/doubler_sparring_MQ2007_general.png} 
  \caption{Doubler Versus Sparring with Preference Based Regret with 46 Arms}
\end{figure}

\newpage
\subsection{Improved Doubler}
Now lets have a look at the Improved Doubler algorithm using the same data set.
\begin{figure}[h!]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.3]{figures/improved_doubler_sparring_MQ2007_7arms.png}
  \caption{7 Arms}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.3]{figures/improved_doubler_sparring_MQ2007_16arms.png}
  \caption{16 Arms}
  \label{fig:sub2}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.3]{figures/improved_doubler_sparring_MQ2007_46arms.png}
  \caption{46 Arms}
  \label{fig:sub2}
\end{subfigure}
\caption{Improved Doubler Versus Sparring with Utility Based Regret}
\label{fig:test}
\end{figure}

The general regret:
\begin{figure}[h!]
  \centering
     \includegraphics[scale=0.3]{figures/improved_doubler_sparring_MQ2007_general.png} 
  \caption{Improved Doubler Versus Sparring with Preference Based Regret with 46 Arms}
\end{figure}
\newpage
\subsection{Balanced Doubler}
Now lets have a look at the Balanced Doubler algorithm using the same data set.
\begin{figure}[h!]
  \centering
     \includegraphics[scale=0.3]{figures/balanced_doubler_sparring_MQ2007_16arms.png} 
  \caption{Balanced Doubler Versus Sparring with Utility Based Regret with 16 Arms}
\end{figure}

\subsection{Thompson Sampling Doubler}
Now lets have a look at the Balanced Doubler algorithm using the same data set.
\begin{figure}[h!]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.3]{figures/improved_doubler_TS_sparring_MQ2007_7arms.png}
  \caption{7 Arms}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.3]{figures/improved_doubler_TS_sparring_MQ2007_16arms.png}
  \caption{16 Arms}
  \label{fig:sub2}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.3]{figures/improved_doubler_TS_sparring_MQ2007_46arms.png}
  \caption{46 Arms}
  \label{fig:sub2}
\end{subfigure}
\caption{Improved Doubler Versus Sparring with Utility Based Regret}
\label{fig:test}
\end{figure}

The general regret:
\begin{figure}[h!]
  \centering
     \includegraphics[scale=0.4]{figures/improved_doubler_TS_sparring_MQ2007_general.png} 
  \caption{Improved Doubler Versus Sparring with Preference Based Regret with 46 Arms}
\end{figure}

\subsection{Thompson Sampling Sparring}
And finally, lets show the results of  the sparring algorithm when using Thompson Sampling black boxes.

\begin{figure}[h!]
  \centering
     \includegraphics[scale=0.4]{figures/TS_sparring_sparring_MQ2007_16arms.png} 
  \caption{Thompson Sampling Sparring Versus Sparring with Utility Based Regret with 16 Arms}
\end{figure}

The general regret:
\begin{figure}[h!]
  \centering
     \includegraphics[scale=0.4]{figures/TS_sparring_sparring_MQ2007_general.png} 
  \caption{Thompson Sampling Sparring Versus Sparring with Preference Based Regret with 46 Arms}
\end{figure}

\end{document}

\documentclass{llncs}
\usepackage{llncsdoc}
\usepackage[utf8]{inputenc}
\usepackage[compatibility=false]{caption}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{dsfont}
\setcounter{secnumdepth}{5}

\author{Amit Wolfenfeld\inst{1}}
\institute{Technion}
\title{Title}

\begin{document}
\maketitle

\begin{abstract}
In machine learning, the notion of multi-armed bandits refers
to a class of online learning problems, in which a learner (also called decision maker or agent) explores and exploits a given set of choice alternatives in the course of a sequential decision process. 
In the standard setting, the learners learns from stochastic feedback in the form of real-valued rewards.

The Dueling Bandits setting is an online learning framework in which actions are restricted to stochastic comparisons between pairs of arms.
It models settings were absolute rewards are difficult to estimate but pairwise preferences are readily available. 

In this paper we propose several new methods for the Dueling Bandit Problem. Our approach extends the Doubler and Sparring algorithm proposed on [???]. We show empirical results using real data from Microsoft Research's LETOR project.
\end{abstract}

\section{Introduction}
	Multi-armed bandit (MAB) algorithms have received considerable attention and have been studied quite intensely in machine learning since the 50's when  Lai and Robbins released their paper [?]. 
	The huge interest in this topic is not very surprising, due to the fact that this MAB setting is not only theoretically challenging but also extremely useful, as can be seen from its use in a wide range of applications. MAB algorithms are used today for solving many problems such as - in search engines [?], online advertisement [?], and recommendation systems [?].
	The multi-armed bandit problem, or bandit problem for short, is one of the simplest instances of the sequential decision making problem, in which a learner needs to select options from a given set of alternatives repeatedly in an online manner -  the name comes from the gambling world in which a gambler decides from a row of slot machines (sometimes known as "one-armed bandits") and decides which machines to play, how many times to play each machine and in which order to play them. When played, each machine provides a random reward from a distribution specific to that machine. 
	The objective of the gambler is to maximize the sum of rewards earned through a sequence of lever pulls. 
	To be more precise, the learner selects one option at a time and observes a numerical (and typically stochastic) reward, providing information on the quality of that arm. The goal of the learner is to optimize an evaluation criterion such as the error rate (the expected percentage of playing a suboptimal arm) or the cumulative regret (the expected difference between the sum of the rewards actually obtained and the sum of rewards that could have been obtained by playing the best arm in each round).
	In order to minimize the regret, the learner has to face the crucial tradeoff at each trial between "exploitation" of the machine that has the apparent highest expected payoff and "exploration" to get more information about the expected payoffs of the other machines. 
	The learner has to find the best "ratio" between playing the arms that produced high rewards in the past (exploitation) and trying other, possibly even better arms the (expected) reward of which is not precisely known so far (exploration).
	There are many variations of the MAB problem and in most of the we assume a numerical reward such as "arm  number 1 has the value of 0.7", however there are many applications where such assumption does not hold,  were the feedback is a pairwise comparison "arm number 1 is better than arm number 2" as opposed to standard bandits.
	There are many cases in the world of machine learning where precise feedback is not available, and only preference feedback is available. In these cases weakly supervised learning and preference learning must be used, and this is what we study here.  		 
	In preference learning, feedback is typically represented in a purely qualitative way, namely in terms of pairwise comparisons or rankings. 
	Feedback of this kind can be useful in online learning, too, as has been shown in online information retrieval.
	Web search and internet marketing are two examples that show the importance for the Dueling Bandits setting. A search engine needs to give the user the best result for his or her query. For every query the search engine lets the user select from several options of search results and receives the feedback according to the users choice. This feedback comes in the form of "the first result is preferred over the other results".  
	Another example is when an advertiser aims to sell products online. The advertiser will direct users to his sale page. 
	Every advertiser wants sell as much as possible therefore they will want to improve their sale page. In order to improve their sale page the advertiser will create several versions of the pages, split the users between them, and see which one is the top performer. This process is called A/B testing, and each page version is represented by an arm in the Multi Armed Bandit  setting. 
	The problem with standard bandits is that there are trends in the market that temporarily decrease or increase the overall performance (Christmas time for instance). 
	Assuming that the arm's order of performance stays the same, meaning the best arm, performance-wise, stays first, the second best arm stays second and so on - Dueling Bandits Algorithms can be used to increase the advertiser's sale performance while keeping the regret to a minimum.	
	Extending the multi-armed bandit setting to the case of preference-based feedback, i.e., the case in which the online learner is allowed to compare arms in a qualitative way, is therefore a promising idea. 
	Indeed, extensions of that kind have received increasing attention in the recent years. 
	The aim of this paper is to provide a survey of the state-of-the-art in the field of Dueling Bandits Algorithms and present several new algorithms. 
	In section 2 we provide a scientific background for the Dueling Bandits Problem. In section 3 we survey the state-of-the-art algorithms. In section 4 we present a new algorithm that out performs the algorithm described in section 3, In section 5 we present the empirical results.
\newpage

\section{Scientific Background}
	In this section we will go into more detail of what the MAB problem is and more importunately the definition of the Dueling Bandits Problem. 
	We discuss two types of settings, the first - Utility Based Dueling Bandit (UBDB) setting and the second Preference Based Dueling Bandit (PBDB) setting.
	\subsection{Multi Armed Bandits}
	As described in the previous section the multi armed bandit problem is a sequential decision making problem, where a learner explores and exploits a stochastic environment. 
	In this setting, the learner performs actions, referred to as arm pulls. 
	The arms belong to an infinite or finite set $X$. 
	If the set is finite we denote $|X|$ by $K$. 
	Each arm $x \in X$ is associated with a probability distribution over $[0, 1]$, with expectation $\mu_{x}$. 
	Throughout this paper we assume the existence of a unique best arm:
	\begin{equation}\label{eq:regret}
		x^* = argmax_{x \in X}(\mu_{x})
	\end{equation}		
	At each round $t > 0$ the learner "pulls" an arm $x_t \in X$ and acquires a stochastic reward or utility $u_t$, independently of all previous rewards (i.i.d). 
	For each arm $x$ and for all rounds $t \geq 1$,  $n_{x}(t)$ denotes the number of times arm $x$ has been "played".
	In this setup the cumulative regret is defined as following:
	\begin{equation}\label{eq:regret}
		R(T) = \sum_{t=1}^{T} \mu(x^*) - u_t
	\end{equation}
	The cumulative regret shows the difference between the utility the player could have acquired if he played the best arm and the sum of utilities actually acquired.
	
	\subsubsection{UCB Algorithm}	
	The most comonly used algorithm for the MAB setting ,the UCB algorithm, is used to minimize the cumulative regret for finite horizon case. 
	It relies on finding an equal probability upper bound for all arms, as seen in line~\ref{line:ref:UCB}, according to the samples so far, called Upper Confidence Bound (hence the name). 
	The bound for arm $x$ decreases as the number of times it had been sampled so far $n_x$ grows.
	At each round, the sampled arm is the arm that potentially has the highest reward, either because it hadn’t been sampled enough, or because the arm’s average reward has been promising.
	
	The UCB algorithm gives the following guarantees:
	
	
\begin{theorem}\label{thm:UCB}[From \cite{...}]

		Running the UCB algorithm with $|X|=K$, with a finite time horizon of $ T > K$, the expected regret is bounded by $R(T) = \mathcal{O} \left( \left(\sum_{x\in X \backslash x^*} \frac{1}{\Delta_x} \right)\log T \right)$, where $\Delta_x$ is the gap between the best arm $x^*$ and arm $x$.

	\end{theorem}		
	Here we present a variant of UCB that uses an additional parameter $ \beta $ to balance between the exploration and exploitation of the algorithm:
	\begin{figure}[h]
	\IncMargin{1em}
		\begin{algorithm}[H]
		\SetKwInput{KwInput}{Input}
		
		\KwInput{$ X,K=|X|$}
			$ t\leftarrow 1$\\
			$\forall x \in X : \hat{\mu_x} \leftarrow 0$\\
			\BlankLine
			
			\While{$t\leq T $}{
				$x_t\leftarrow argmax_{x\in X} \left(\hat{\mu}_x + \beta \sqrt{\frac{\ln t}{n_x}}\right)$ \label{line:ref:UCB}\\
 				play arm $x_t$ and acquire $u_t$\\
 				update $\hat{\mu}_x$
			}
			\caption{UCB}
		\end{algorithm}
		\caption{UCB Algorithm}\label{algo_UCB}
	\end{figure}

	\subsubsection{Thompson Sampling Algorithm}	
	One of the earliest algorithms, given by W. R. Thompson, dates back to 1933. The basic idea is to choose an arm to play according to its probability of being the best arm.
	The Thompson Sampling algorithm proceeds as follows.
	The algorithm maintains the number of successes and failures for each arm, and holds a random variable with $\beta$-distribution for each arm $\Theta_x\sim Beta(Success_x+1,Failures_x+1)$.
	At each round all the random variables $\Theta_{x,t}$ are sampled.
	The chosen arm is then given by $ x_t \in argmax_{x\in X} \Theta_{x,t}$. 
	While the theoretical behaviour of Thompson sampling has remained elusive for a long time, fairly good understanding of its theoretical properties was achieved by Agrawal and Goyal [?] the first logarithmic regret bound was proved: 
	
	\begin{theorem}\label{thm:TS}[From \cite{...}]

		Running the Thompson Sampling algorithm with $|X|=K$, with a finite time horizon of $ T > K$, the expected regret is bounded by $R(T) = \mathcal{O} \left( \left(\sum_{x\in X \backslash x^*} \frac{1}{\Delta_x^2} \right)^2 \log T \right)$, where $\Delta_x$ is is the gap between the best arm $x^*$ and arm $x$.

	\end{theorem}	
	The pseudo-code of the Thompson Sampling algorithm can be seen in Figure \ref{algo_TS}:
	
	\begin{figure}[h]
	\IncMargin{1em}
		\begin{algorithm}[H]
		\SetKwInput{KwInput}{Input}
		
		\KwInput{$ X,K=|X|$}
			$ t\leftarrow 1$\\
			$\forall x \in X : Success_{x} \leftarrow 0$\\
			$\forall x \in X : Fails_{x} \leftarrow 0$\\			
			\BlankLine
			
			\While{$t\leq T $}{
				$\forall x \in X : \Theta_{x,t} \sim Beta(Success_{x}+1, Fails_{x}+1)$\\
				$x_t\leftarrow argmax(\Theta_{x,t})$\\
 				play arm $x_t$ and acquire $u_t$\\
 				$Success_{x_t} \leftarrow Success_{x_t} + u_t$\\
 				$Fails_{x_t} \leftarrow Fails_{x_t} + (1-u_t)$
			}
			\caption{Thompson Sampling}
		\end{algorithm}
		\caption{Thompson Sampling Algorithm}\label{algo_TS}
	\end{figure}	
	
	\subsection{Dueling Bandits}
		To formalize the problem of learning from preferences, we consider the following interactive online learning model,for the K-armed dueling bandit problem (Yue et al., 2012).
		At each iteration $t$, the learning system presents two arms $x_t, y_t \in X$ to the learner, where $X$ is the set (either finite or infinite) of possible actions. The two arms are compared and the feedback comes in the form of a binary random variable $b_t$, declaring which arm beats the other arm.
		In this paper we will study two different problem settings.
		The first \textbf{UBDB} Utility Based Dueling Bandits, where each arm $x_t$, $y_t$ acquires a utility, unobservable by the learner, and the feedback $b_t$ "behaves" according to the utilities. 
		The second - \textbf{PBDB} - Preference Based Dueling Bandits, where the outcome of the comparison between the arms behaves according to a predefined preference matrix $P$ that characterises the relationship between the arms.
	\subsubsection{UBDB}
		In this setting each of the two arms has an associated random reward (or utility) for the learner, which we denote by $u_t$ and $v_t$, respectively.
		The quantity $u_t$ (resp. $v_t$) is drawn from a distribution that depends on $x_t$ (resp. $y_t$) only.  
		We assume, as always, these utilities are in $[0, 1]$. 
		The learner is rewarded the average utility $ U_{av}(t) = (u_t + v_t)/2$ \footnote{ where \textit{av} is shorthand for average} of the two actions it presents, but it does not observe this reward. 
		Instead, it only observes the a binary choice among the two alternative arms $x_t, y_t$, which depends on the respective utilities $u_t$ and $v_t$. 
		In particular, we model the observed choice as a binary random variable $b_t$ distributed according to:
		\begin{equation}\label{eq:b_t}
		\begin{cases}
		P(b_t = 1|u_t, v_t) = \phi(u_t, v_t)
		\\
		P(b_t = 0|u_t, v_t) = \phi(v_t, u_t)
		\end{cases}
		\end{equation}			
	where $\phi:[0, 1] \times [0, 1] \rightarrow [0, 1]$ is a link function. 
	Clearly, the link function has to satisfy $\phi(A, B) + \phi(B, A) = 1$.
	$b_t$ can be viewed as an indicator for the event $x_t \succ y_t $, or in words "$x_t$ beats $y_t$ in round $t$".
	We assume $u_t$ is drawn from a distribution of expectation $\mu(x_t)$ and $v_t$ independently from a distribution of expectation $\mu(y_t)$.
	The link function $\phi$, which is assumed to be known, quantitatively determines how to translate the utilities $u_t, v_t$ to winning probabilities. 
	The linear link function $\phi_{lin}$ is defined by
	\begin{equation}\label{eq:link_function}
		P(b_t = 1|u_t, v_t) = \phi_{lin}(u_t, v_t) = \frac{1+v_t-u_t}{2}\in [0,1]
	\end{equation}
	For the UBDB case the definition of the regret is very natural and straight forward:

	\begin{equation}\label{eq:utility_regret}
		R_U(T) = \sum_{t=1}^T \mu(x^*)-U_{av}(t)
	\end{equation}	
	Where, as always,  $x^* = argmax_{x \in X} \mu(x)$. 
	This implies that expected zero regret is achievable by setting $(x_t, y_t) = (x^*,x^*)$.
	It should be also clear that playing $(x^*,x^*)$ is pure exploitation, because the feedback is then an unbiased coin with zero exploratory information.
	
	\subsubsection{PBDB}	
		Same as in the UBDB setting	consider a fixed set of arms $X = \{x_1,..,x_k\}$. 
		As actions, the learner performs a comparison between any pair of arms $x_t$ and $y_t$ , meaning the action space is identified with the set of index pairs $(i, j)$ such that $1 \leq i \leq j \leq K$. 
		In this paper we characterise the feedback of the comparison by an unknown preference matrix $P$, which is not necessarily devised from the latent utility.
		More precisely
		
		\begin{equation}\label{eq:preference_matrix}
			P = [p_{x,y}] \in [0,1]^{K \times K} 
		\end{equation}
		To be more precise - for each pair of arms $(x ,y)$, this relation specifies the probability of the event
		\begin{equation}\label{eq:p_xy}
			Pr(x \succeq y) = p_{x, y}
		\end{equation}
		of observing a preference for $x$ in a direct comparison with $y$. 
		Meaning, each $p_{x,y}$ defines a Bernoulli distribution.
		Throughout this paper we assume the these probabilities are independent and stationary during all rounds $t > 0$.		
		This means that whenever two arms is played $(x ,y)$ and compared, the outcome is distributed according to (1), without any dependencies on the previous iterations.
		The relation matrix $P$ is reciprocal in the sense that $p_{x,y} = 1-p_{x,y}$ for all $x,y \in X$.		
		Arm $x$ is said to outperform arm $y$ if $p_{x,y} > 1/2$, meaning the probability of winning in a pairwise comparison is larger for $x$ than it is for $y$ . 
		The closer $p_{x,y}$ is to $1/2$, the harder it is to distinguish between arm $x$ and arm $y$ based on a finite sample set from $Pr(x \succeq y)$. This resembles the case in the standard MAB problem where the gap $\Delta_{x,y}$ is very small.
		 When $p_{x,y} = 1/2$, the learner cannot decide which arm is better based on a finite number of pairwise comparisons.
		 Defining a regret is more tricky in PBDB. In particular, it is necessary to make assumptions on $P$ for the definition to make sense.
		 In (Joachims and Yue, 2011) "Relaxed Stochastic Transitivity" is assumed, defined by:
		 For any triplet of arms $x \succ y \succ z$ and some $ \gamma \geq 1 $, 
		 we assume $\gamma \Delta_{x,z} \geq \max(\Delta_{x,y},\Delta_{y,z})$.
		 Where $\Delta_{x,y}$ is defined
		 \begin{equation}\label{eq:delta}
		 	\Delta_{x,y} = p_{x,y} - 1/2
		 \end{equation}		 		 
		 In a later paper (Urvoy et al., 2013) this assumption was relaxed, and only a Condorcet winner is assumed, where a Condercet winner is defined as an arm $x$, such that $ \forall y, p_{x,y} > 1/2$. 
		 Given a Condorcet winner, we define regret for each time-step as follows (Yue et al., 2012): if arms $x$ and $y$ were chosen for comparison at time $t$, then regret at that time is $ \frac{\Delta_{x^*,x_t}+\Delta_{x^*,y_t}}{2} $ and the cumulative regret is
		 \begin{equation}\label{eq:preference_regret}
		 R_P(T) = \sum_{t=1}^T \frac{\Delta_{x*,x_t}+\Delta_{x*,y_t}}{2}
		 \end{equation}
%		 Where $\Delta_{x,y}$ is defined		 
%		 $$ \Delta_{x,y} = p_{x,y} - 1/2$$
%		 This to be a good quantity to characterize the "damage" that is inflicted for each choice the learner makes during the Dueling Bandits game.
		 As opposed to the standard bandit game $\Delta_{x,y}$ can be negative, in which the quantity used for the multi-armed bandit task is always positive and depends on the gap between the means of the best arm and the suboptimal arms.
		 
	\subsection{Probability Estimation}		 
		The Dueling Bandit game is played in discrete rounds, either through a finite time horizon or an infinite horizon. 
		 As described in the previous section, the learner compares between two arms in each round $t > 0$. 
		 And so, in each round $t$, the learner selects a pair of arms $x_t,y_t$ and observes
	\begin{equation}
		\begin{cases}
    		x_t \succeq y_t & \text{with probablity } p_{x_t,y_t} 
       	\\
    		y_t \succeq x_t & \text{with probablity } p_{y_t,x_t}
	\end{cases}
	\end{equation}		
	In this paper the pairwise probabilities $p_{x,y}$ can is estimated according to the finite sample sets.
	We consider the set of rounds among the first $t$ iterations, in which the learner decides to compare arms $x$ and $y$ , and denote the size of this set by $n_{x,y}$, or the number of times $x$ and $y$ have been compared.  
	We denote the number of times $x$ "beat" over $y$ by $w_{x,y}$ and $w_{y,x}$ the number of "beat" of $y$ over $x$.
	It is easy to see that $n_{x,y} = n_{y,x} = w_{x,y}+w_{y,x}$ and so the unbiased estimation of $p_{x,y}$ up to iteration $t$ is then given by
	\begin{equation}\label{eq:est_pxy}
		\hat{P}_{x, y} = \frac{w_{x,y}}{n_{x,y}} = 
		\frac{w_{x,y}}{w_{x,y}+w_{y,x}}
	\end{equation}	 
	As mention above, in this paper we assume that the samples are independent and identically distributed (i.i.d), $\hat{p}_{i,j}$ is a good estimate of the pairwise probability (2). 
	As in most MAB algorithms a high probability confidence interval is obtained by the Hoeffding bound defined in Theorem~\ref{thm:Hoeffding}. 
	The confidence intervals may differ from one algorithm to another, but usually it is of the form $[p_{x,y} \pm c_{x,y} ]$. 
	According to this definition arm $x$ outperforms arm $y$ with high likelihood if $p_{x,y} + c_{x,y} > 1/2]$, and, $x$ is beaten by arm $y$ with high probability, if $p_{x,y} + c_{x,y} < 1/2]$.
	\subsubsection{Hoeffding Inequality}	
	As a preliminary step, we remind Hoeffding’s inequality.
	\begin{theorem}\label{thm:Hoeffding}[From \cite{...}]
	Suppose $\{X_1,..,X_N \}$ are independent random variables with values in the interval $[a,b]$.
	We denote $\bar{X}_N = \sum_{i=1}^N \frac{X_i}{N}$.\\
	If $E[\bar{X}_N] = \mu_X$, then for any $d>0$:
	\begin{subequations}
	\begin{equation}
	Pr(\bar{X}_N \geq \mu_X+ d) \leq e^{-\frac{2d^2}{(b-a)^2}\cdot\frac{1}{N}}
	\end{equation}
	\begin{equation}
	Pr(\bar{X}_N \leq \mu_X- d) \leq e^{-\frac{2d^2}{(b-a)^2}\cdot\frac{1}{N}}
	\end{equation}
	\end{subequations}
	\end{theorem}
	\subsection{The Relation Between Preference and Utility based Regrets}
	
	In the extreme case where the preference matrix $P$ is induced by \textbf{UBDB} we argue that the regret defined in  section 2.2.1 is the same as in  section 2.2.2. 
	We will show that using the definition of the linear link function both utility based regret and preference based regret are the same (upto a factor of 2): 
	\begin{equation}\label{eq:ref1}
		p_{x^*,y} = \phi_{lin}(\mu(x^*),\mu(y)) = \frac{1 +\mu(y)-\mu(x^*)}{2}
	\end{equation}
	Incorporating \eqref{eq:ref1} in the definition of $\Delta_{x,y}$ we get
	$$
	\Delta_{x^*,y} = p_{x^*,y} - \frac{1}{2} = \frac{\mu(x^*)-\mu(y)}{2}
	$$
	And so the total regret is defined:
	$$ R_P(T) = \sum_{t=1}^T \frac{\Delta_{x^*,x_t}+\Delta_{x^*,y_t}}{2} =  
\sum_{t=1}^T \frac{\frac{\mu(x^*)-\mu(x_t)}{2}+\frac{\mu(x^*)-\mu(y_t)}{2}}{2} =
$$

$$
\frac{1}{2} \sum_{t=1}^T \mu(x^*) -\frac{
	\mu(x_t)+\mu(y_t)}{2} =
\frac{1}{2} \sum_{t=1}^T \mu(x^*)- U_{av} = \frac{1}{2}R_U(T)$$

\subsection{Table of Notations}
For the convenience of the reader we have included a table of all the definitions. 
	\begin{table}[h]
		\begin{tabular}{ll}
 			$T$ & Horizon \\
 			$t$ &  Round \\
 			$X$ & Arms space \\
 			$K = |X|$ & Total Number of Arms\\
 			$x_t \in X$ & Left arm \\
 			$y_t \in X$ & Right arm \\
 			$u_t \in \mu_t$ & Left utility \\
 			$v_t \in \mu_t$ & Right utility \\
 			$b_t$ & Observed feedback \\
 			$R_U(T)$ & Total Utility Based Regret till T\\
			$R_P(T)$ & Total Preference Based Regret till T\\
 			$\mathcal{P}$ & Set of potential arms \\
 			$\hat{P}_{x, y}$ & Estimate of $P(x>y)$\\
 			$\hat{C}_{x, y}$ &   Confidence interval of - $(\hat{P}_{x, y} - \sqrt{log(1/\delta)/t},\hat{P}_{x, y} +\sqrt{log(1/\delta)/t})$\\
 			$n_x$ &   The number of times arm $x$ has been played\\
 			$w_x$ & The number of times arm $x$ has won\\
 			$\hat{P}_x  $ &  $ w_x / n_x $\\
 			$ W = [w_{x,y}]$ & Number of wins of arm $x$ over arm $y$\\
 			$U = [u_{x,y}]$ &  Utility function\\
 			$\Theta_{x,y}$ &   Random variable with Beta distribution.
		\end{tabular}
	\end{table}

\section{Survey of Algorithms for Dueling Bandits}
	We start by surveying dueling bandit algorithms. In this section we have included Interleaved Filtering, Beat the Mean Bandit, RUCB, RCS, SAVAGE and the Sparring and Doubler algorithms for they served as a foundation for our new approach. We categorize these algorithm based on the following criteria - Explore and Exploit algorithms and Pure Exploitation algorithms.

\subsection{Explore and Exploit Algorithms}
	In the finite horizon case most PBDB algorithms are based on the idea of splitting the rounds into two phases, exploration and then exploitation. In the first phase the algorithm identifies the best arm with high probability. In the second phase the algorithm repeatedly compares the chosen arm to itself.
	Such algorithm, using the above principle are called "explore-then-exploit" algorithms.
	The main draw back for these algorithm is that $T$, the horizon, is needed to be known in advance. The horizon is needed for the algorithm to control the trade-off between exploration and exploitation and to control the regret accumulated in the event of failure to identify the best arm.
	To be more specific, let's assume the algorithm can identify the best arm with probability of at least $1-\delta$. We will set $\delta$ to $1/T$ and so the algorithm can guarantee that the best arm is chosen within $T$ rounds with probability greater then $1-1/T$.
	Let's define $R_{explore}(T),R_{exploit}(T)$, the regrets for the exploration phase and exploitation phase, that define the total regret:
	$$R(T) = R_{explore}(T)+R_{exploit}(T)$$
	Assuming the algorithm made a mistake and did not choose the best arm at the end of exploration phase, the regret accumulated , $R_{explore}(T)+R_{exploit}(T)$ is at most $\mathcal{O}(T)$, since the regret accumulated in each round is bounded by 1.
	Assuming the algorithm did choose the best arm $R_{exploit}(T)$ is $0$.
	Consequently, the expected regret of an explore-then-exploit algorithm is
	\begin{equation}
		E[R(T)] \leq (1-1/T)E[R_{explore}(T)] +(1/T)\mathcal{O}(T) = \mathcal{O}(E[R_{explore}(T)]+1)
	\end{equation}
	The same argument holds for the case of high probability regret bounds in the explore-then-exploit framework. 
	In summary, the performance of an explore-then-exploit algorithm is bounded by the performance of the exploration algorithm. 
	Although Explore and Exploit algorithms need to know the Horizon $T$ in advance, by using a simple workaround ,the "Squaring Trick", these algorithm can be used in the infinite horizon setting. An estimated horizon $\hat{T}$ is submitted to the algorithm and once the algorithm reaches $t=\hat{T}$ the horizon is set to $\hat{T}^2$, this process continues till the algorithm terminates, and hence the name "Squaring Trick".
	
	\subsection{Pure Exploration Algorithms}
	In the infinite time horizon case , in which no horizon is specified and the evaluation process continues indefinitely.
	In this case it is not sufficient for the algorithm to maximize accuracy or minimize regret after a single horizon is reached. 
	Instead, it must minimize regret by rapidly decreasing the frequency of comparisons involving suboptimal arms, particularly those that fare worse in comparison to the best arm.
	In the standard bandit problems, the arm that is chosen in each round produces an expected reward that is at most $\epsilon$ shy of the optimal arm, the $\epsilon$-optimal arm.
	In the PBDB case approximation errors are harder to define.  
	A preference-based MAB algorithm is an ($\epsilon, \delta$)-PAC PBDB algorithm with a sample complexity $B(P,\epsilon,\delta)$, if it returns the $\epsilon$-optimal arm with probability at least $1-\delta$ , and the number of comparisons taken by the algorithm is at most $B(P,\epsilon,\delta)$.
	The sample complexity of the learner is then the number of pairwise comparisons it makes before concluding the optimal arm, and the corresponding bound is denoted $B(P,\epsilon,\delta)$ where $P$ is defined at \eqref{eq:preference_matrix}.

	\subsection{The Algorithms}
\subsubsection{Interleaved Filter}
	Yue et al.[?] propose an algorithm for a finite horizon setting. 
	Their algorithm works for the scenario in which the preference matrix satisfies the following properties:

	\begin{enumerate}

		\item Strong stochastic transitivity.

		\item Stochastic triangle inequality

	\end{enumerate}
	Strong stochastic transitivity means that for any triplet of arms $x \succ y \succ z$ we assume $\Delta_{x,z} \geq \max(\Delta_{x,y},\Delta_{y,z})$.
	Stochastic triangle inequality means that for any triplet of arms $x \succ y \succ z$ we assume $\Delta_{x,z} \leq \Delta_{x,y} + \Delta_{y,z}$.
	The regret is defined as in \eqref{eq:preference_regret}.
	The algorithm implements an explore-then-exploit strategy. 
	This means that the time steps $1..T$ are divided into two phases, the first serving for exploration only (learning) and the second for exploitation only. 
	In particular, in the exploitation phase, the algorithm plays only the apparent optimal action consistently.
	The exploration phase works by repeated elimination as follows. 
	At each step the algorithm maintains a working set of arms that are potential optimal. 
	It is shown that with high probability the true optimal arm is consistently in the working set. 
	The elimination is done using a round robin tournament strategy, as shown in line~\ref{lst:line:rr} of algorithm~\ref{algo_IF}. 
	Each arm in the working set is compared against all the arms in the working set, until sufficient confidence is achieved to determine whether the arm must be eliminated (removed from the working set). 
	By sufficient confidence we mean that with high probability an arm cannot be the optimal. This is encoded in the following prediction(line~\ref{lst:line:elimination}):
	 $$p_{x,y}+c_{x,y}<1/2$$
	Apart from the round robin elimination, a pruning process is executed.
	We explain the pruning process more precisely. 
	As can be seen in line~\ref{lst:line:pruning}, an element $y$ is removed from the working set $\cal P$ if $$p_{x,y}-c_{x,y}<1/2$$ 
	In words, this condition implies that $y$ is with high confidence inferior compared to the current candidate in the round robin.
	Once a single arm is left in the working set, the algorithm enters the exploitation phase.
	The algorithm pseudo-code is presented in Figure~\ref{algo_IF}. 
	The guarantee provided is as follows:

	\begin{theorem}\label{thm:IF}[From \cite{...}]

		Running the Interleaved Filter algorithm with $|X|=K$, with a known finite time horizon of $T>K$, the expected regret is bounded by $R_P(T) = \mathcal{O} \left( \frac{K\log K}{\Delta_{x^*,y}} \log T \right)$, where $y$ is an arm that is only beaten by $x^*$.

	\end{theorem}
	From the analysis of the algorithm it can be seen that the algorithm returns the best arm with probability of $1-\frac{1}{T}$.
	Correspondingly, a suboptimal arm is returned by the algorithm with probability of $\frac{1}{T}$ hence , similar to a PAC type argument: With high probability of at least $1-\frac{1}{T}$, the estimated regret after $T$ steps is at most $\mathcal{O} \left( \frac{K\log K}{\Delta^*} \log T \right)$. The regret is defined as in \eqref{eq:preference_regret}.
	
	\begin{figure}[h]
	\IncMargin{1em}
		\begin{algorithm}[H]
		\SetKwInput{KwInput}{Input}
		
		\KwInput{$T$,$ X,K=|X|, \delta$}
			$ t\leftarrow 1$\\
			$ \mathcal{P} \leftarrow X$
			Choose $x_t \in \mathcal{P}$ randomly\\
			$ \mathcal{P} \leftarrow \mathcal{P} \backslash x_t$\\
			\BlankLine
			\While{$|\mathcal{P}|>1 $}{

 				\For{$x \in \mathcal{P}$\label{lst:line:rr}}{
					compare($y, x_t$)\\
					update($\hat{P}_{x_t, y}$)\\
					$t \leftarrow t+1$	
				}
				$c_t \leftarrow \sqrt{log(1/\delta)/t}$\\
				\While{$\exists y \in$ s.t. $\left( \hat{P}_{x_t,y}+c_t<1/2 \right)$\label{lst:line:elimination}}{
				$\mathcal{P} \leftarrow \mathcal{P} \backslash \{y\}$
				}
				\If{$\exists y \in$ s.t. $( \hat{P}_{x_t,y}-c_t <1/2 )$\label{lst:line:pruning}}{
					$x_{t+1} \leftarrow y$\\
					$\mathcal{P} \leftarrow \mathcal{P} \backslash \{ y \}$\\
					$\forall x \in \mathcal{P}$ reset $\hat{P}_{y,x}$
				}
			}
			\Return{$x_t \in \cal P$}
			\caption{Interleaved Filter}
		\end{algorithm}
		\caption{Interleaved Filter Algorithm}\label{algo_IF}
	\end{figure}
	

\subsubsection{Beat The Mean Bandit}
	Yue and Joachims [?] proposed an explore-then-exploit strategy, for a finite horizon setting, very similar to IF. 
	Beat the Mean (BTM) - a preference-based online learning algorithm.
	This algorithm is based on relaxed assumptions with regards to the IF algorithm:
	\begin{enumerate}
		\item Relaxed stochastic transitivity.
		\item Stochastic triangle inequality
	\end{enumerate}
	Relaxed stochastic transitivity means that for any triplet of arms $x \succ y \succ z$ and for some $ \gamma \geq 1 $ we assume $\gamma\Delta_{x,z} \geq  \max(\Delta_{x,y},\Delta_{y,z})$.
	The main idea of BTM is to maintain a score of the "Mean Bandit" (arm in our case) and eliminating arms that are inferior to it with enough confidence.
	Similar to the IF algorithm BTM maintains a working set of potentially optimal arms.
	At each round the algorithm picks an arm that has the least plays in the corresponding current history, $n_x$, and compares it with a randomly chosen arm from the working set.
	Comparing an arm against random arms in the working set is functionally identical to comparing an arm against the "Mean Bandit" sampled uniformly from the working set.
	The arm, $x$,  that is empirically worst arm and is separated enough from the best arm with enough confidence is defined using:
	\begin{equation}\label{eq:BTM_el}
	min_{x\in \mathcal{P}} (\hat{P}_{x})+c^* \leq max_{y \in \mathcal{P}}(\hat{P}_{y})-c^*
\end{equation}	 
	Once an arm reached \eqref{eq:BTM_el} (see line~\ref{alg:BTM:elimination}) it is eliminated from the working set. The confidence is defined as $ c^* = c_{\delta, \gamma}(n^*) $, where $c_{\delta, \gamma}(n) = 3\gamma^2 \sqrt{\frac{1}{n}\log \frac{1}{\delta}}$.
	The algorithm pseudo-code is presented in Figure~\ref{algo_BTM}.	
	The guarantee provided is as follows:
	\begin{theorem}\label{thm:BTM}[From \cite{...}]

		Running the Beat the Mean Bandit algorithm with $|X|=K$, with a known finite time horizon of $T>K$, the expected regret is bounded by $R_P(T) = \mathcal{O} \left( \frac{\gamma^7K}{\Delta_{x^*,y}} \log T \right)$, where $y$ is an arm that is only beaten by $x^*$.

	\end{theorem}	
	Similarly to the analysis of the IF algorithm it can be seen that the algorithm returns the best arm with probability of $1-\frac{1}{T}$.	
	
	\begin{figure}[h]
	\IncMargin{1em}
		\begin{algorithm}[H]
		\SetKwInput{KwInput}{Input}
		
		\KwInput{$T$,$X,K=|X|, c_{\delta, \gamma}(\cdot)$}
			$ \mathcal{P} \leftarrow X $\\
			$ W = [w_{x,y}]$\\
			$ \forall x \in \mathcal{P} , n_x \leftarrow 0 $\\
			$ \forall x \in \mathcal{P} , w_x \leftarrow 0 $\\
			$ \forall x \in \mathcal{P} ,\hat{P}_x \leftarrow w_x / n_x$ or $0$ if $n_x = 0 $\\
			$t \leftarrow 0$ \\
			\BlankLine
			\While{$|\mathcal{P}|>1$}{
				$x_t \leftarrow argmin_{x \in \mathcal{P}} n_x$\\
				select $y_t\in \mathcal{P}$ at random\\
				compare($x_t$, $y_t$)\\
				update($W$)\\
				$n_{x_t} \leftarrow n_{x_t}+1$\\ 
				$ n^*  \leftarrow \min_{x \in \mathcal{P}} n_x$\\
				$ c^* \leftarrow c_{\delta, \gamma}(n^*) $ or $1$ if $ n^* = 0$\\
				\If{$min_{y\in \mathcal{P}} (\hat{P}_{y})+c^* \leq max_{x \in \mathcal{P}}(\hat{P}_{x})-c^*$ \label{alg:BTM:elimination}}{
					$y\leftarrow argmin_{x\in \mathcal{P}}\hat{P}_{x}$\\
					$\forall x \in \mathcal{P}$ delete comparison with $y$ from $w_y, n_y$\\
					$\mathcal{P} \leftarrow \mathcal{P}\backslash\{y\} $\\
				} 
			$t \leftarrow t+1$
			}
			\Return{$x_t \in \cal P$}
			\caption{Beat the Mean Bandit}
		\end{algorithm}
		\caption{Beat the Mean Bandit Algorithm}\label{algo_BTM}
	\end{figure}

	\subsubsection{RUCB} Relative Upper Confidence Bound by Zoghi et al. [?], adapts the most commonly used algorithm UCB [Algorithm~ \ref{algo_UCB}] in the standard MAB setting to the Dueling Bandit setting (PBDB version).
	The algorithm assumes that a Condorcet winner exists.
	A Condorcet winner is a definition borrowed from social choice \cite{work by condorcet}.  Roughly speaking, a Condorcet winner beats all the other arms on average. Mathematically, a Condorcet winner (if exists) is defined by: $x$ is the Condorcet winner if $ p_{x,y} > 1/2, \forall y $.
	The algorithms pseudo-code is presented in Figure~\ref{algo_RUCB}.
	Similarly the the UCB algorithm the RUCB algorithm maintains an upper confidence bound $u_{x_t,y_t}$, for each pair of arms, as seen in line~\ref{alg:rucb:line:ucb_set} of Algorithm~\ref{algo_RUCB}.
	On each round, RUCB selects the arms to compare from the set of potential Condorcet winners, meaning the set of arms for which all are potentially Condorcet winners with sufficiently high probability. 
	After selecting the set of potential Condercet winners the algorithm selects the pair of arms with the highest upper confidence bound $u_{x_t,y_t}$, this can be seen as a Dueling Bandit version of the "optimism in the face of uncertainty" principle from UCB.
	Note that in line~\ref{alg:rucb:line:ucb_choose_set} the comparisons are based on $u_{x_t,y_t}$ and in line~\ref{alg:rucb:line:ucb_choose} of Algorithm~\ref{algo_RUCB} the comparisons are based on $u_{y_t,x_t}$ making it more difficult for the algorithm to compare an arm against itself.
	Thus, RUCB strives to avoid auto-comparisons (comparisons of an item with itself) until there is great certainty that $x_t$ is indeed the Condorcet winner.
	Roughly speaking, if the algorithm would pick the arm $y_t$ according to $u_{x_t,y_t}$ and not $u_{y_t,x_t}$, the algorithm will pick the arm according to the impact of $x_t$ and not $y_t$ and is more likely to pick the pair $x_t,y_t = x_t,x_t$. 
	The RUCB algorithm guarantees the following:
	\begin{theorem}\label{thm:BTM}[From \cite{...}]

		Running the RUCB algorithm with $|X|=K$, with a finite time horizon of $T>K$, the expected regret is bounded by $R_P(T) = \mathcal{O} \left( \frac{K log T}{\Delta_{x^*,y}} \right)$, where $y$ is an arm that is only beaten by $x^*$.

	\end{theorem}
	
	\begin{figure}[h]
	\IncMargin{1em}
		\begin{algorithm}[H]
		\SetKwInput{KwInput}{Input}
		
		\KwInput{$T$,$X,K=|X|, \alpha >1/2,T \in \{1,2,... \}$}
		$ W = [w_{x,y}]$\\
		\BlankLine
		\For{$t =1,2..,T $}{
			$U = [u_{x,y}] \leftarrow \frac{w_{x,y}}{w_{x,y}+w_{y,x}} +\sqrt{\frac{\alpha \cdot \ln(t)}{w_{x,y}+w_{y,x}}}$ \label{alg:rucb:line:ucb_set}\\
			$\forall x \in X : u_{x,x} \leftarrow 0 $\\
			Pick any $x_t$ that satisfies $u_{x_t,x} \geq 1/2, \forall x$. \label{alg:rucb:line:ucb_choose_set}\\
			If no $x_t$ exists pick $x_t$ randomly from $X$.\\
			$y_t \leftarrow argmax_{y} u_{y,x_t}$\label{alg:rucb:line:ucb_choose}\\
			compare($x_t$ , $y_t$)\\
			update($W$) 
} 
			\caption{RUCB}
		\end{algorithm}
		\caption{RUCB Algorithm}\label{algo_RUCB}
	\end{figure}
	
	\newpage
\subsubsection{RCS}
	(Munos et al., 2014) Relative Confidence Sampling is very similar to RUCB in the sense that it aims to minimize cumulative regret and does not eliminate arms from a working set.
	The algorithms pseudo-code is presented in Figure~\ref{algo_RCS}.
	Similarly to the RUCB algorithm, the RCS algorithm's only assumption is the existence of a Condorcet winner.
	
	On each round the algorithm simulates a tournament between all the arms, against all the arms. 
	The simulation is conducted, as seen in line~\ref{alg:rcs:simulation}, according to a random variable $\Theta_{x,y} $	with $Beta(\alpha,\beta)$ distribution. 
	The hyper-parameters, $\alpha,\beta$, are $w_{x,y}+1,w_{y,x}+1$ accordingly. Where $w_{x,y}$ is the number of times arm $x$ beat arm $y$.
	The main idea behind this simulation technique is to use the superior performance of Thompson Sampling [?], in the same manner it is used in the K-armed bandit setting. 
	Once the simulation is over, similarly to the RUCB algorithm, the RCS algorithm select a set of potential Condorcet winners, as seen in line~\ref{alg:rcs:select_x}, and by this the algorithm selects arm $x_t$.
	In line~\ref{alg:rcs:select_y} the algorithm selects the arm $y_t$ according to the "optimism in the face of uncertainty" principle same as the UCB algorithm.
	After both arms are selected and compared, the algorithm updates $w_{x,y}$ for all the arms and continues to the next round. 
	Although no theoretical guarantees where presented in [?], according to the experiments there, RCS outperforms the RUCB algorithm for a small number of arms (less than 40).	
	In Figure~\ref{alg:RCS:exp} we can see the results of the experiments Munos et al. performed on the MSLR-WEB30K dataset.
	To evaluate the RCS algorithm, Munos et al. applied it to the problem of ranker evaluation from the field of information retrieval (IR) (Manning et al., 2008). 
	A ranker is a function that takes a user's search query and ranks the documents in a collection according to their relevance to that query. 
	Ranker evaluation aims to determine which set of rankers performs best.
	An effective way to achieve this is to use interleaved comparisons (Radlinski et al., 2008), which interleave the documents proposed by two different rankers and presents the resulting list to the user, whose resulting click feedback is used to infer a stochastic preference for one of the rankers. 
	Given a set of K rankers, the problem of finding the best ranker can then be modelled  as a K-armed Dueling Bandit problem, with each arm relating to a ranker.
	These experiments are built on real IR data from the MSLR-WEB30K dataset.
	Using this data set, 20,30 and 40 rankers, each relating to a ranking feature provided in the data set, e.g., PageRank. 
	The ranker evaluation task then corresponds to conclude which single feature is the best ranker (Hofmann et al., 2013).
	
	\begin{figure}[h]
	\includegraphics[scale=0.2]{RCS.jpg}
	\caption{Cumulative regret averaged over 90 runs}\label{alg:RCS:exp}
	\end{figure}
	\begin{figure}[h]
	\IncMargin{1em}
		\begin{algorithm}[H]
		\SetKwInput{KwInput}{Input}
		
		\KwInput{$T$,$X,K=|X|, \alpha >1/2,T \in \{1,2,... \}$}
		$ W = [w_{x,y}]$\\
		\BlankLine
		\For{$t =1,2..,T $}{
	%$\Theta(t) = \frac{\mathds{1}_{K\times K}}{2}$\\
	\emph{assume an arbitrary injective mapping index$:X\rightarrow[K]$, \\
	inducing an arbitrary canonical order on $X$.}\\
	\For{$x,y \in X$ s.t. $index(x)<index(y)$ \label{alg:rcs:simulation}}{
		 $\Theta_{x,y}(t) \sim Beta(w_{x,y}+1,w_{y,x}+1 )$\\
		 $\Theta_{y,x}(t) = 1-\Theta_{x,y}(t)$
		 }
	Pick any $x_t$ that satisfies $\Theta_{x_t,x} \geq 1/2, \forall x$.\label{alg:rcs:select_x}\\ 
	If no $x_t$ exists pick $x_t$ that was chosen least often ($argmin_x n_x)$.\\
	$U = [u_{x,y}]\leftarrow \frac{w_{x,y}}{w_{x,y}+w_{y,x}} +\sqrt{\frac{\alpha \cdot \ln(t)}{w_{x,y}+w_{y,x}}}$ \\
	$U_{z,z} \leftarrow 0 $ for each $z \in X$\\
	$y_t \leftarrow argmax_{y} u_{y,x_t}$\label{alg:rcs:select_y}\\
	compare($x_t$, $y_t$)\\
	update($W$) 
} 
			\caption{RCS}
		\end{algorithm}
		\caption{RCS Algorithm}\label{algo_RCS}
	\end{figure}	
	
	\newpage
	
	
	
\subsubsection{SAVAGE}
	(Uryoy et al., 2013) Sensitivity Analysis of Variables for Generic Exploration (SAVAGE) algorithm is another explore-then-exploit algorithm, as are BTM and IF.
	As in RUCB and RCS this algorithm runs under the assumption that a Condorcet winner exists.
	The algorithms pseudo-code is presented in Figure~\ref{algo_SAVAGE}
	In general, the algorithm compares  pairs arms until there exists a pair for which one of the arms beats the other by a wide margin, in which case the loser is removed from the working set.
	More precisely, in each round two arms, with the least amount of plays, are chosen, as seen in line~\ref{alg:savage:select_xy}.
	The pair of arms are compared and given a score, $u_{x_t, y_t}$ (Line~\ref{alg:savage:score_xy}).
	An arm $x$ that does not uphold the condition
	$$ \sum_{y}\mathbbm{1}(u_{x,y} > 0.5) < K $$
	is eliminated from the working set.
	This condition is very similar to the condition in the RUCB algorithm.
	Although this algorithm seems like "more of the same", however this algorithm eliminates arms from the working set (as seen in IF and BTM) according to the likelihood of the arm being the Condorcet winner, while RUCB and RCS only pick the most likely pair of arms to hold the Condorcet winner.
	This algorithm guarantees the following:
	\begin{theorem}\label{thm:SAVAGE}[From \cite{...}]

		Running the SAVAGE algorithm with $|X|=K$, with an unknown infinite time horizon of $T>K$, the expected regret is bounded by $$R_P(T) = \mathcal{O} \left( K^2 log( T ) \sum\limits_{x\in X \backslash x^*} \frac{1}{\Delta_{x,x^*}^2}\right)$$

	\end{theorem}
	
\begin{figure}[h]
	\IncMargin{1em}
		\begin{algorithm}[H]
		\SetKwInput{KwInput}{Input}
		
		\KwInput{$X ,K=|X|, \alpha$}
			$ \mathcal{P} \leftarrow X $\\
			$ \forall x \in \mathcal{P} , n_x \leftarrow 0 $\\
		\BlankLine
		\While{$|\mathcal{P}|>1$}{			
			$x_t, y_t \leftarrow  argmin_{x}{n_{x}}$\label{alg:savage:select_xy}\\
			Compare($x_t, y_t$)\\
			$n_{x_t} \leftarrow n_{x_t}+1$\\
			$n_{y_t} \leftarrow n_{y_t}+1$\\
			update($W$)\\
			$U = [u_{x_t, y_t}]\leftarrow \frac{w_{x,y}}{w_{x,y}+w_{y,x}} +\sqrt{\frac{\alpha}{w_{x,y}+w_{y,x}}}$\label{alg:savage:score_xy}\\
			$\mathcal{P}  \leftarrow \mathcal{P}\backslash \left\{x\mid 				\sum_{y}\mathbbm{1}(u_{x,y} > 0.5) < K)\right\}$\\
			$t \leftarrow t+1$
		}
		\Return{ $x_t $}
			\caption{SAVAGE}
		\end{algorithm}
		\caption{SAVAGE Algorithm}\label{algo_SAVAGE}
	\end{figure}
	\newpage
	
\subsubsection{Black-Box Algorithms}	
	In \cite{??}, Ailon et al. tried to tackle the dueling bandits problem (UBDB-version) using standard bandit algorithms.
	In their work, Ailon et al. use MAB algorithm black boxes, defined as Singleton Bandits Machine (SBM) - closed computational units with internal timer and memory.
	A SBM $S$ supports three operations: reset, advance and feedback. The reset operation simply clears the SBM's state.
	The advance operation returns the next chosen arm, and feedback is used for simulating a feedback (the utility).
	\paragraph{Doubler}
	(Ailon et al., 2014) handles a large or possibly infinite set of arms $X$ for the infinite horizon case.
	The algorithms pseudo-code is presented in Figure~\ref{algo_Doubler}.
	This algorithm is best explained by thinking of a competition between two players; the first controls the choice of the left arm and the second player controls the right arm. 
	The objective of each player is to win as many rounds possible.
	This algorithm divides the time axis to exponentially growing epochs  $p$ (First epoch is 2 rounds, second epoch is 4, third epoch is 8 and so forth).
	In each epoch, the left side plays according to a fixed stochastic strategy, which will be explained shortly, while the right one plays adaptively according to a strategy provided by a SBM black-box.
	In the beginning of a new epoch (line~\ref{line:new_epoch}) the distribution governing the left arm changes in a way that mimics the actions of the right arm in the previous epoch.
	More precisely, the left side plays according to a distribution defined by the multi-set $\mathcal{L}$ that holds all the plays the right side performed in the previous epoch.
	The algorithm guarantees the following:
	\begin{theorem}\label{thm:Doubler}[From \cite{...}]

		Running the Doubler algorithm with $|X|=K$, with an infinite time horizon, the expected regret is bounded by $R_U(T) = \mathcal{O} \left(H log^2 T \right)$ for all $T>0$, where $H=\sum_{x\in X \backslash x^*}^K \Delta^{-1}_x$ and the regret as in \eqref{eq:utility_regret}, and $\Delta_x$ is the gap between arm $x$ and the optimal arm.

	\end{theorem}
	As can be seen in line~\ref{line:new_epoch} of Algorithm~\ref{algo_Doubler},  $\mathcal{S}$ is reset every time a new epoch begins, by these removing all the information gained in the previous rounds. This procedure inflicts the extra $Log(T)$ factor to the accumulated regret.
	
	\begin{figure}[h]
	\IncMargin{1em}
		\begin{algorithm}[H]
		\SetKwInput{KwInput}{Input}
		
		\KwInput{$X$, $\mathcal{S}$}
		\BlankLine
		$p\leftarrow 1$\\
		$ \mathcal{L} \leftarrow X$\\
		\While{True}{
			reset($\mathcal{S}$) \label{line:new_epoch}\\
			\For{$t = 2^{p-1}$ \KwTo $2^{p}$}{
				choose $x_t$ uniformly from $\mathcal{L}$\\
				$y_t \leftarrow advance(\mathcal{S})$\\
				compare $(x_t , y_t )$, observe choice $b_t$\\
				feedback $\left(\mathcal{S},b_t\right)$ \label{alg:dblr:feedback}\\
			}
			$\mathcal{L} \leftarrow$the multi-set of arms played as y t in the last for-loop.\\
		$p \rightarrow p+1$
	}
			\caption{Doubler}
		\end{algorithm}
		\caption{Doubler Algorithm}\label{algo_Doubler}
	\end{figure}

\paragraph{Sparring}
	(Ailon et al., 2014) The algorithms pseudo-code is presented in Figure~\ref{algo_Sparring}.
	The idea for the Sparring algorithm comes from analysis of an adversarial version of UDBD, in which it can be easily shown that the resulting expected regret of Sparring is at most a constant times the regret of the two SBM black-boxes which implement an algorithm for adversarial MAB (i.e $EXP3$).
		
		\begin{figure}[h]
	\IncMargin{1em}
		\begin{algorithm}[H]
		\SetKwInput{KwInput}{Input}
		\KwInput{$X, \mathcal{L} = \{x_1\}$, $ \mathcal{S}_R, \mathcal{S}_L$}
		\BlankLine
		$t\leftarrow 1$\\
		\While{True}{
			$x_t\leftarrow$ advance($\mathcal{S}_L$),
			$y_t\leftarrow$ advance($\mathcal{S}_R$)\\
			compare($x_t, y_t$)\\
			observe $b_t$\\
			feedback($\mathcal{S}_L$, $\mathbbm{1}_{b_t=0}$)\\
			feedback($\mathcal{S}_R$, $\mathbbm{1}_{b_t=1}$)\\	
			$t\rightarrow t+1$\\
		}
			\caption{Sparring}
		\end{algorithm}
		\caption{Sparring Algorithm}\label{algo_Sparring}
	\end{figure}	
	
		\input{Sparring.tex}
	\begin{conjecture}
 		In the paper Ailon et al., 2014 conjectured that the utility based regret of the Sparring algorithm is bounded by the combined regret of the SBM black-boxes, with a possibility of a small overhead.
 	\end{conjecture}	
 	Proving the conjecture appears to be tricky due to the fact
that the left (resp. right) SBM black-box does not see a stochastic environment, because its feedback depends on non-stochastic actions of the two SBM black-boxes. 
	
\section{Our Approach}	
	
	\subsection{Improved Doubler}
	The main drawback of the Doubler algorithm is that in each epoch the SBM is reset and as a result all the acquired data is lost, thereby adding an extra $log(T)$ factor to the total regret.
	The algorithms pseudo-code is presented in Figure~\ref{algo_adv_doubler}.
	Instead of initializing the right arm’s SBM at each epoch and losing all the data, we will store this data.
	Let $D_p$ denote the distribution of the left arm in the $p$’th epoch. 
	Had we known $f_p := E_{x\in D_p} \mu(x)/2$, we could have used $b_t + f_p$ as feedback on the right side.
	This would have offset the change in the expected utility of the left arm, from epoch to epoch.
	As a result, we would not have ad to reset the SBM at each epoch.
	By changing line~\ref{alg:dblr:feedback} in the Doubler algorithm to feedback($\mathcal{S}, b_t + f_p $).
	It is easy to see that $E(b_t + f_p )$ is exactly $E(\frac{u(y_t)+1}{2})$, meaning we have decoupled the arms' utilities.
	Of course we do not know $f_p$, and can only estimate it.
	Luckily, an unbiased estimate of $f_p$, $\hat{f}_p$ can be obtained using a simple inductive formula.
	First we notice $f_p$ is defined by 
	\begin{equation}
	f_{p+1} = \frac{1}{|T_p|}\sum\limits_{t\in T_p} \mu(y_t)/2
	\end{equation}
	and so:
	$$
	f_{p+1} = 
		\frac{1}{|T_p|}\sum\limits_{t\in T_p} E[b_t - \frac{1-\mu(x_t)}{2}] = \frac{1}{|T_p|}\sum\limits_{t\in T_p} E[b_t - f_{p-1}-\frac{1}{2}]
	$$
	The estimate for $f_p$ is given by a recursive formula:
	\begin{equation}
	\hat{f}_p=\hat{f}_{p-1} + \frac{\sum\limits_{t\in T_p} (2b_t -1)}{|T_{p-1}|}
	\end{equation}
	We wish to bound $\hat{f}_p$:
	
	$$\hat{f}_{p+1}=\hat{f}_{p} + 
		\frac{\sum\limits_{t\in T_p} (2b_t -1)}{|T_{p}|} = 
		\hat{f}_{p} + 
		\frac{\sum\limits_{t\in T_p} (2b_t -1)}{2^{p}} = 
		\hat{f}_{p} + 
		\frac{\sum\limits_{t\in T_i} 2b_t}{2^{p}}-1 =$$
		$$
		\hat{f}_{p} + B_p -1 =  
		\hat{f}_{p-1} +B_{p-1} -1+ B_p -1 = ... =  
		\hat{f}_{0} + \sum\limits_{j=1}^p B_{j}+p$$
	
	$\sum\limits_{j=1}^p B_{j}$  equals $E[b]$ which is bounded by $[0,1]$ and $p$ which is $log(T)$. And so we can bound $\hat{f}$ by $\log(T)$.
	This is a very rough bound of $\hat{f}_p$, with it we cannot prove that the total regret achieve a better bound then $\mathcal{O}(log^2(T))$.
	In our experiments we have witnessed that $\hat{f}_p$ is far smaller then the rough bound and is bounded by $\mathcal{O}(1)$, that proves that the total regret is bound by $\mathcal{O}(log(T))$.
	We have yet to prove this and leave this for further research.
	
\begin{figure}[h]
	\IncMargin{1em}
		\begin{algorithm}[H]
		\SetKwInput{KwInput}{Input}
		\KwInput{$X, \hat{f_0} = 0$, $\mathcal{S}$}
		\BlankLine
		$\mathcal{L} \leftarrow X$\\
		$p\leftarrow 1$\\
\While{True}{
	\For{$t = 2^{p-1}$ \KwTo $2^{p}$}{\label{forins}
		choose $x_t$ uniformly from $\mathcal{L}$\\
		$y_t \leftarrow advance(\mathcal{S})$\\
		play $(x_t , y_t )$, observe choice $b_t$\\
		feedback $\left(\mathcal{S},b_t + \hat{f}_{p-1}\right)$\\
	}
	$\mathcal{L} \leftarrow$the multi-set of arms played as y t in the last for-loop.\\
	$\hat{f_{p}} \leftarrow \hat{f}_{p} + \sum_{s\in T_p} b_s /2^{p-1} - 1/2$\\
	$p \rightarrow p+1$
}
			\caption{Improved Doubler}
		\end{algorithm}
		\caption{Improved Doubler Algorithm}\label{algo_adv_doubler}
	\end{figure}		
	
		

	\subsection{Sparring with Thompson Sampling}
		In our work  we have experimented with both SBMs black and white boxes. The configuration that showed the best result with minimum regret (the results will be presented in the next chapter). 
		The algorithms pseudo-code is presented in Figure~\ref{algo_SparringTS}.
		The algorithm maintains the number of times each arm won and failed a comparison.
		In each round a simulation is conducted for each of the arms.
		The simulation is according to the random variables $\Theta_{x,R}, \Theta_{x,L}$ (for each arm $x \in X$) with beta distribution. 
		The beta distribution is defined by the hyper-parameters $Success_{L,x}$, $Fails_{L,x}$ for the left set of random variable set and $Success_{R,x}, Fails_{R,x}$ for the right set(lines~\ref{alg:sts:Theta_L}, \ref{alg:sts:Theta_R}).
		After the arms are compared and the outcome is observed all the relevant hyper-parameters are updated with the results.
		
	\begin{figure}[h]
	\IncMargin{1em}
		\begin{algorithm}[H]
		\SetKwInput{KwInput}{Input}
		
		\KwInput{$X$, $\mathcal{S}_R, \mathcal{S}_L$}
		\BlankLine
		$t\leftarrow 1$\\
\While{True}{
	$\forall x \in X \Theta_{L,x} \sim Beta(Success_{L,x}+1, Fails_{L,x}+1)$ \label{alg:sts:Theta_L}\\
	$\forall x \in X \Theta_{R,x} \sim Beta(Success_{R,x}+1, Fails_{R,x}+1)$ \label{alg:sts:Theta_R}\\
	$x_t\leftarrow argmax_{x\in X}(\Theta_{L,x})$\\
	$y_t\leftarrow argmax_{x\in X}(\Theta_{R,x})$\\
	compare($x_t, y_t$)\\
	observe $b_t$\\
	$Success_{L,x_t} \leftarrow Success_{L,x_t} + \mathbbm{1}_{b_t=0}$\\
	$Fails_{L,x_t} \leftarrow Fails_{L,x_t} + \mathbbm{1}_{b_t=1}$\\
	$Success_{R,y_t} \leftarrow Success_{R,y_t} + \mathbbm{1}_{b_t=1}$\\
	$Fails_{R,y_t} \leftarrow Fails_{R,y_t} + \mathbbm{1}_{b_t=0}$\\	
	$t\rightarrow t+1$\\
}
			\caption{Sparring Thompson Sampling}
		\end{algorithm}
		\caption{Sparring Thompson Sampling Algorithm}\label{algo_SparringTS}
	\end{figure}
					
	\subsection{Unforgetful Thompson Sampling Doubler}
		Another improvement of the Doubler algorithm is the Unforgetful Thompson Sampling Doubler algorithm - here we do not reset the SBM in each epoch and use the Thompson Sampling method. This approach outperformed all the other algorithms apart from the Thompson Sampling Sparring.
		The algorithms pseudo-code is presented in Figure~\ref{algo_uts_doubler}.

		\begin{figure}[h]
	\IncMargin{1em}
		\begin{algorithm}[H]
		\SetKwInput{KwInput}{Input}
		\KwInput{$X$, $\mathcal{S}$}
		\BlankLine
		$\mathcal{L} \leftarrow X$\\
		$p\leftarrow 1$\\
		\While{True}{
			\For{$t = 2^{p-1}$ \KwTo $2^{p}$}{
				choose $x_t$ uniformly from $\mathcal{L}$\\
				$\forall x \in X \Theta_{R,i} \sim Beta(Success_{R,x}+1, Fails_{R,x}+1)$\\
				$y_t\leftarrow argmax_{x \in X}(\Theta_{R,x})$)\\
				compare $(x_t , y_t )$\\
				observe choice $b_t$\\
				$Success_{R,y_t} \leftarrow Success_{R,y_t} + \mathbbm{1}_{b_t=1}$\\
				$Fails_{R,y_t} \leftarrow Fails_{R,y_t} + \mathbbm{1}_{b_t=0}$\\	
			}
			$\mathcal{L} \leftarrow$the multi-set of arms played as $y_t$ in the last for-loop.\\
			$p \rightarrow p+1$
}
			\caption{Unforgetful Thompson Sampling Doubler}
		\end{algorithm}
		\caption{Unforgetful Thompson Sampling Doubler}\label{algo_uts_doubler}
	\end{figure}
		
		\newpage	
	\subsection{Balanced Doubler}
		Another improvement of the Doubler algorithm is the Balanced Doubler algorithm.In this algorithm we assume we know $min_{x\in X \backslash x^*}(\Delta_x)$
		
		\begin{figure}[h]
	\IncMargin{1em}
		\begin{algorithm}[H]
		\SetKwInput{KwInput}{Input}
		\KwInput{$X$, $\mathcal{S}$}
		\BlankLine
		$\mathcal{L} \leftarrow X$\\
		$p\leftarrow 1$\\
		\While{True}{
		Play each arm $\frac{k}{min_{x\in X \backslash x^*}(\Delta_x)}$ times against all other arms\\
		\For{$t = 2^{p-1}$ \KwTo $2^{p}$}{
		choose $x_t$ uniformly from $\mathcal{L}$\\
		$y_t \leftarrow argmax_i
			\left(
				\frac{1}{p}\sum\limits_{j=1}^{p}\frac{1}{n_{i,j}}
						   \sum\limits_{t\in T_{i,j}} b_t
				+
				\sqrt{
					\alpha\cdot\log(t)
					\cdot(
						\sum\limits_{j=1}^{p}\frac{1}{n_{i,j}}
						)/p^2
					}
			\right)$\\
		compare $(x_t , y_t)$\\
		observe choice $b_t$\\
		}
	$\mathcal{L} \leftarrow$the multi-set of arms played as $y_t$ in the last for-loop.\\
	$p \rightarrow p+1$
}
			\caption{Balanced Doubler}
		\end{algorithm}
		\caption{Balanced Doubler}\label{algo_balanced_doubler}
	\end{figure}
		
		Proof of $log(T)$ regret:
		\\
		Lets define : $Y_i(t) = \frac{1}{p}\sum\limits_{j=1}^{p}\frac{1}{n_{i,j}}
						   \sum\limits_{t\in T_{i,j}} b_t$		
		\\
		Where $n_{i,j}$ is the number of time arm $i$ is played in epoch $j$.\\
		$X_i = Y_i (b_t = b_t- f_j) =   \frac{1}{p}\sum\limits_{j=1}^{p}\frac{1}{n_{i,j}}
						   \sum\limits_{t\in T_{i,j}} b_t-f_j$	
		\\
		Now let's look at $\mathbbm{E}(e^{\lambda X_i})$
		\\	
		$\mathbbm{E}(e^{\lambda X_i}) = \mathbbm{E}(e^{\lambda\cdot(
		\frac{1}{p}\sum\limits_{j=1}^{p}\frac{1}{n_{i,j}}
						   \sum\limits_{t\in T_{i,j}}( b_t-f_j
						   )}) =						      	   
						   \prod\limits_{j=1}^{p}
						   \left(
						   \mathbbm{E}\left[e^{\frac{\lambda}{p}\cdot\left(
						   	   \sum\limits_{t\in T_{i,j}}
						   	   		\frac{(b_t-f_j)}{n_{i,j}}
						\right)}\right]
						   \right)=
						   $		
						   \\
						   $ =						      	   
						   \prod\limits_{j=1}^{p}
						   \prod\limits_{t\in T_{i,j}}
						   \left(
						   \mathbbm{E}\left[e^{\frac{\lambda}
						   {p}\cdot\left(
						   	   		\frac{(b_t-f_j)}
						   	   		{n_{i,j}}\right)}
						   	   		\right]
						   \right)$
		Assuming that $\frac{1}{p}\cdot\left(\frac{b_t-f_j}
		{n_{i,j}}\right) $ is bounded by 1 and has an average of 0, we can say that
		\\
		$$ \prod\limits_{j=1}^{p}
						   \prod\limits_{t\in T_{i,j}}
						   \left(
						   \mathbbm{E}\left[e^{\frac{\lambda}
						   {p}\cdot\left(
						   	   		\frac{b_t-f_j}
						   	   		{n_{i,j}}\right)}
						   	   		\right]
						   \right) \leq
						   \prod\limits_{j=1}^{p}
						   \prod\limits_{t\in T_{i,j}}
						   \left(
						   e^{\frac{\lambda^2}{p^2\cdot n^2_{i,j}}}
						   \right)
						   $$
						   
						  There are $n_{i,j}$ plays in $T_{i,j}$, and so we get:
						  \\
						  $\prod\limits_{j=1}^{p}
						   \prod\limits_{t\in T_{i,j}}
						   \left(
						   e^{\frac{\lambda^2}{p^2\cdot n^2_{i,j}}}
						   \right) = 
						   \prod\limits_{j=1}^{p}						   
						   \left(
						   e^{\frac{\lambda^2}{p^2\cdot n_{i,j}}}
						   \right) = 
						   e^{
						   \frac{\lambda^2}{p^2}
						   \sum\limits_{j=1}^{p} \frac{1}{n_{i,j}}
						   }$
						
		And now for the main point, we wish to show that 
		$Pr[X_j > a] < \frac{1}{t}$\\
		$Pr[X_j > a] = Pr[e^{X_j} < e^a] \leq e^{
						   \frac{\lambda^2}{p^2}
						   \sum\limits_{j=1}^{p} \frac{1}{n_{i,j}}
						   - \lambda\cdot a}$
		\\
		\\
		$\lambda = \frac{a}{\frac{2}{p^2}\left( \sum\limits_{j=1}^{p} \frac{1}{n_{i,j}} \right)} $ 
		\\
		with some reverse engineering and in order to get $Pr[X_j > a] < \frac{1}{t}$ we get that $a = \sqrt{\frac{\log(t) \sum\limits_{j=1}^{p} \frac{1}{n_{i,j}}}{p^2}}$ 
 
 
	
		
\section{Experiments}

To evaluate our algorithms, we apply them to the problem of
ranker evaluation from the field of information retrieval
(IR) (Manning et al., 2008). A ranker is a
function that takes a user's search query and
ranks the documents in a collection according to their
relevance to that query. Ranker evaluation aims to determine
which set of rankers performs best.
\\
An effective way to achieve this is to use interleaved
comparisons (Radlinski et al., 2008), which interleave
the documents proposed by two different rankers and
presents the resulting list to the user, whose resulting
click feedback is used to infer a stochastic preference
for one of the rankers. Given a set of K rankers, the
problem of finding the best ranker can then be modelled 
as a K-armed Dueling Bandit problem, with each
arm relating to a ranker.
\\
Our setup is built on real IR data,
namely the LETOR MQ2007 dataset (Liu et al., 2007).
Using this data set, we create a set of 46 rankers, each
relating to a ranking feature provided in the
data set, e.g., PageRank. The ranker evaluation task
then corresponds to conclude which single feature
is the best ranker (Hofmann et al., 2013).
\\
We evaluated our algorithms and RCS
using randomly chosen subsets from the pool of 46
rankers, yielding K-armed dueling bandit problems
with $K\in \{7, 16, 46\}$. 
\\
We will present the utility based regret and the general regret (as define in section 2.1 and 2.2) on the MQ2007. 
\newpage

\subsection{Results}
We will start with displaying the results for 7 arms.
From Figure 1 we can clearly see that the SAVAGE and Doubler algorithms are outperformed by the other algorithms and so will be left out from the next figures.
\begin{figure}[h!]
\centering
  \includegraphics[scale=0.5, natwidth=610,natheight=642]{figures/all_MQ2007_7arms.png}
  \caption{Utility Based Regret with 7 Arms}
\end{figure}

For a larger number of arms we can see that RCS and RUCB are outperformed by the other algorithms.

\begin{figure}[h!]
\centering
  \includegraphics[scale=0.5, natwidth=610,natheight=642]{figures/all_MQ2007_24arms.png}
  \caption{Utility Based Regret with 24 Arms}
\end{figure}

And for 46 arms we can see that Sparring with Thompson Sampling methods performs better than all the other algorithms.


\subsection{Sparring}

We can see the clear advantage of using the Sparring algorithm over the RCS algorithm when dealing with a large number of arms.
\begin{figure}[h!]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.3, natwidth=410,natheight=442]{figures/rcs_sparring_MQ2007_7arms.png}
  \caption{7 Arms}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.3, natwidth=410,natheight=442]{figures/rcs_sparring_MQ2007_16arms.png}
  \caption{16 Arms}
  \label{fig:sub2}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.3, natwidth=410,natheight=442]{figures/rcs_sparring_MQ2007_46arms.png}
  \caption{46 Arms}
  \label{fig:sub2}
\end{subfigure}
\caption{RCS Versus Sparring with Utility Based Regret}
\label{fig:test}
\end{figure}


With a small number of arms (Fig 1.) we can observe that the RCS algorithm quickly converges to the optimal arm.
\\
With a larger number of arms we can see that Sparring out-performs the RCS algorithm. 
\\
Now lets look at the general regret (as defined section 2.2). Here we used 46 arms:
\begin{figure}[h!]
  \centering
     \includegraphics[scale=0.4, natwidth=410,natheight=442]{figures/rcs_sparring_MQ2007_general.png} 
  \caption{RCS Versus Sparring with Preference Based Regret with 46 Arms}
\end{figure}


\newpage
\subsection{Doubler}
Now lets have a look at the Doubler algorithm using the same data set.
\begin{figure}[h!]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.3, natwidth=410,natheight=442]{figures/doubler_sparring_MQ2007_7arms.png}
  \caption{7 Arms}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.3, natwidth=410,natheight=442]{figures/doubler_sparring_MQ2007_16arms.png}
  \caption{16 Arms}
  \label{fig:sub2}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.3, natwidth=410,natheight=442]{figures/doubler_sparring_MQ2007_46arms.png}
  \caption{46 Arms}
  \label{fig:sub2}
\end{subfigure}
\caption{Doubler Versus Sparring with Utility Based Regret}
\label{fig:test}
\end{figure}

The general regret:
\begin{figure}[h!]
  \centering
     \includegraphics[scale=0.2, natwidth=410,natheight=442]{figures/doubler_sparring_MQ2007_general.png} 
  \caption{Doubler Versus Sparring with Preference Based Regret with 46 Arms}
\end{figure}

\newpage
\subsection{Improved Doubler}
Now lets have a look at the Improved Doubler algorithm using the same data set.
\begin{figure}[h!]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.3, natwidth=410,natheight=442]{figures/improved_doubler_sparring_MQ2007_7arms.png}
  \caption{7 Arms}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.3, natwidth=410,natheight=442]{figures/improved_doubler_sparring_MQ2007_16arms.png}
  \caption{16 Arms}
  \label{fig:sub2}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.3, natwidth=410,natheight=442]{figures/improved_doubler_sparring_MQ2007_46arms.png}
  \caption{46 Arms}
  \label{fig:sub2}
\end{subfigure}
\caption{Improved Doubler Versus Sparring with Utility Based Regret}
\label{fig:test}
\end{figure}

The general regret:
\begin{figure}[h!]
  \centering
     \includegraphics[scale=0.3, natwidth=410,natheight=442]{figures/improved_doubler_sparring_MQ2007_general.png} 
  \caption{Improved Doubler Versus Sparring with Preference Based Regret with 46 Arms}
\end{figure}
\newpage
\subsection{Balanced Doubler}
Now lets have a look at the Balanced Doubler algorithm using the same data set.
\begin{figure}[h!]
  \centering
     \includegraphics[scale=0.3, natwidth=410,natheight=442]{figures/balanced_doubler_sparring_MQ2007_16arms.png} 
  \caption{Balanced Doubler Versus Sparring with Utility Based Regret with 16 Arms}
\end{figure}

\subsection{Thompson Sampling Doubler}
Now lets have a look at the Balanced Doubler algorithm using the same data set.
\begin{figure}[h!]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.3, natwidth=410,natheight=442]{figures/improved_doubler_TS_sparring_MQ2007_7arms.png}
  \caption{7 Arms}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.3, natwidth=410,natheight=442]{figures/improved_doubler_TS_sparring_MQ2007_16arms.png}
  \caption{16 Arms}
  \label{fig:sub2}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[scale=0.3, natwidth=410,natheight=442]{figures/improved_doubler_TS_sparring_MQ2007_46arms.png}
  \caption{46 Arms}
  \label{fig:sub2}
\end{subfigure}
\caption{Improved Doubler Versus Sparring with Utility Based Regret}
\label{fig:test}
\end{figure}

The general regret:
\begin{figure}[h!]
  \centering
     \includegraphics[scale=0.4, natwidth=410,natheight=442]{figures/improved_doubler_TS_sparring_MQ2007_general.png} 
  \caption{Improved Doubler Versus Sparring with Preference Based Regret with 46 Arms}
\end{figure}

\subsection{Thompson Sampling Sparring}
And finally, lets show the results of  the sparring algorithm when using Thompson Sampling black boxes.

\begin{figure}[h!]
  \centering
     \includegraphics[scale=0.4, natwidth=410,natheight=442]{figures/TS_sparring_sparring_MQ2007_16arms.png} 
  \caption{Thompson Sampling Sparring Versus Sparring with Utility Based Regret with 16 Arms}
\end{figure}

The general regret:
\begin{figure}[h!]
  \centering
     \includegraphics[scale=0.4, natwidth=410,natheight=442]{figures/TS_sparring_sparring_MQ2007_general.png} 
  \caption{Thompson Sampling Sparring Versus Sparring with Preference Based Regret with 46 Arms}
\end{figure}

\end{document}
